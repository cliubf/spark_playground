{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local spark setup with ipython\n",
    "\n",
    "## prerequisite：\n",
    "\n",
    "> ipython in your local PC, [anaconda](https://www.anaconda.com/download/) recommended\n",
    "\n",
    "> spark in your local PC, extracted to directory. [Download page](https://spark.apache.org/downloads.html)\n",
    "\n",
    "> Powershell (For Windows)\n",
    "\n",
    "## command to run\n",
    "\n",
    "1. initialize ipython from your cmd\n",
    "```bat\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "2. install packages if required (findspark for example)\n",
    "```bat\n",
    "pip install findspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important: please modify the path as your local spark location \n",
    "spark_path = \"D:/spark-2.3.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark is well set at 2018-07-29 09:45:41.774597\n"
     ]
    }
   ],
   "source": [
    "# configure spark, a message regrads time will print out if success\n",
    "import findspark\n",
    "from datetime import datetime\n",
    "findspark.init(\"D:/spark-2.3.1-bin-hadoop2.7\")\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ibm_hr\")\n",
    "sc = SparkContext(conf = conf)\n",
    "print(\"spark is well set at \" + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Chengzhong:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ibm_hr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x23ce60643c8>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open spark session\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "\n",
    "## Disclaimer: \n",
    "* for academic use only\n",
    "* data is available on [Kaggle](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset/kernels)\n",
    "* most of the analysis pipeline in this notebook is referenced from this [kernel](https://www.kaggle.com/arthurtok/employee-attrition-via-rf-gbm). I do not own the copyright. For use other than academic, please contact with original author.\n",
    "\n",
    "### Data loading and quality check\n",
    "* findout if data is loaded correctly\n",
    "* check if there is any null in any columns\n",
    "* Get familar with our target: Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------------+---------+--------------------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+--------------------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "|Age|Attrition|   BusinessTravel|DailyRate|          Department|DistanceFromHome|Education|EducationField|EmployeeCount|EmployeeNumber|EnvironmentSatisfaction|Gender|HourlyRate|JobInvolvement|JobLevel|             JobRole|JobSatisfaction|MaritalStatus|MonthlyIncome|MonthlyRate|NumCompaniesWorked|Over18|OverTime|PercentSalaryHike|PerformanceRating|RelationshipSatisfaction|StandardHours|StockOptionLevel|TotalWorkingYears|TrainingTimesLastYear|WorkLifeBalance|YearsAtCompany|YearsInCurrentRole|YearsSinceLastPromotion|YearsWithCurrManager|\n",
      "+---+---------+-----------------+---------+--------------------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+--------------------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "| 41|      Yes|    Travel_Rarely|     1102|               Sales|               1|        2| Life Sciences|            1|             1|                      2|Female|        94|             3|       2|     Sales Executive|              4|       Single|         5993|      19479|                 8|     Y|     Yes|               11|                3|                       1|           80|               0|                8|                    0|              1|             6|                 4|                      0|                   5|\n",
      "| 49|       No|Travel_Frequently|      279|Research & Develo...|               8|        1| Life Sciences|            1|             2|                      3|  Male|        61|             2|       2|  Research Scientist|              2|      Married|         5130|      24907|                 1|     Y|      No|               23|                4|                       4|           80|               1|               10|                    3|              3|            10|                 7|                      1|                   7|\n",
      "| 37|      Yes|    Travel_Rarely|     1373|Research & Develo...|               2|        2|         Other|            1|             4|                      4|  Male|        92|             2|       1|Laboratory Techni...|              3|       Single|         2090|       2396|                 6|     Y|     Yes|               15|                3|                       2|           80|               0|                7|                    3|              3|             0|                 0|                      0|                   0|\n",
      "| 33|       No|Travel_Frequently|     1392|Research & Develo...|               3|        4| Life Sciences|            1|             5|                      4|Female|        56|             3|       1|  Research Scientist|              3|      Married|         2909|      23159|                 1|     Y|     Yes|               11|                3|                       3|           80|               0|                8|                    3|              3|             8|                 7|                      3|                   0|\n",
      "| 27|       No|    Travel_Rarely|      591|Research & Develo...|               2|        1|       Medical|            1|             7|                      1|  Male|        40|             3|       1|Laboratory Techni...|              2|      Married|         3468|      16632|                 9|     Y|      No|               12|                3|                       4|           80|               1|                6|                    3|              3|             2|                 2|                      2|                   2|\n",
      "| 32|       No|Travel_Frequently|     1005|Research & Develo...|               2|        2| Life Sciences|            1|             8|                      4|  Male|        79|             3|       1|Laboratory Techni...|              4|       Single|         3068|      11864|                 0|     Y|      No|               13|                3|                       3|           80|               0|                8|                    2|              2|             7|                 7|                      3|                   6|\n",
      "| 59|       No|    Travel_Rarely|     1324|Research & Develo...|               3|        3|       Medical|            1|            10|                      3|Female|        81|             4|       1|Laboratory Techni...|              1|      Married|         2670|       9964|                 4|     Y|     Yes|               20|                4|                       1|           80|               3|               12|                    3|              2|             1|                 0|                      0|                   0|\n",
      "| 30|       No|    Travel_Rarely|     1358|Research & Develo...|              24|        1| Life Sciences|            1|            11|                      4|  Male|        67|             3|       1|Laboratory Techni...|              3|     Divorced|         2693|      13335|                 1|     Y|      No|               22|                4|                       2|           80|               1|                1|                    2|              3|             1|                 0|                      0|                   0|\n",
      "| 38|       No|Travel_Frequently|      216|Research & Develo...|              23|        3| Life Sciences|            1|            12|                      4|  Male|        44|             2|       3|Manufacturing Dir...|              3|       Single|         9526|       8787|                 0|     Y|      No|               21|                4|                       2|           80|               0|               10|                    2|              3|             9|                 7|                      1|                   8|\n",
      "| 36|       No|    Travel_Rarely|     1299|Research & Develo...|              27|        3|       Medical|            1|            13|                      3|  Male|        94|             3|       2|Healthcare Repres...|              3|      Married|         5237|      16577|                 6|     Y|      No|               13|                3|                       2|           80|               2|               17|                    3|              2|             7|                 7|                      7|                   7|\n",
      "| 35|       No|    Travel_Rarely|      809|Research & Develo...|              16|        3|       Medical|            1|            14|                      1|  Male|        84|             4|       1|Laboratory Techni...|              2|      Married|         2426|      16479|                 0|     Y|      No|               13|                3|                       3|           80|               1|                6|                    5|              3|             5|                 4|                      0|                   3|\n",
      "| 29|       No|    Travel_Rarely|      153|Research & Develo...|              15|        2| Life Sciences|            1|            15|                      4|Female|        49|             2|       2|Laboratory Techni...|              3|       Single|         4193|      12682|                 0|     Y|     Yes|               12|                3|                       4|           80|               0|               10|                    3|              3|             9|                 5|                      0|                   8|\n",
      "| 31|       No|    Travel_Rarely|      670|Research & Develo...|              26|        1| Life Sciences|            1|            16|                      1|  Male|        31|             3|       1|  Research Scientist|              3|     Divorced|         2911|      15170|                 1|     Y|      No|               17|                3|                       4|           80|               1|                5|                    1|              2|             5|                 2|                      4|                   3|\n",
      "| 34|       No|    Travel_Rarely|     1346|Research & Develo...|              19|        2|       Medical|            1|            18|                      2|  Male|        93|             3|       1|Laboratory Techni...|              4|     Divorced|         2661|       8758|                 0|     Y|      No|               11|                3|                       3|           80|               1|                3|                    2|              3|             2|                 2|                      1|                   2|\n",
      "| 28|      Yes|    Travel_Rarely|      103|Research & Develo...|              24|        3| Life Sciences|            1|            19|                      3|  Male|        50|             2|       1|Laboratory Techni...|              3|       Single|         2028|      12947|                 5|     Y|     Yes|               14|                3|                       2|           80|               0|                6|                    4|              3|             4|                 2|                      0|                   3|\n",
      "| 29|       No|    Travel_Rarely|     1389|Research & Develo...|              21|        4| Life Sciences|            1|            20|                      2|Female|        51|             4|       3|Manufacturing Dir...|              1|     Divorced|         9980|      10195|                 1|     Y|      No|               11|                3|                       3|           80|               1|               10|                    1|              3|            10|                 9|                      8|                   8|\n",
      "| 32|       No|    Travel_Rarely|      334|Research & Develo...|               5|        2| Life Sciences|            1|            21|                      1|  Male|        80|             4|       1|  Research Scientist|              2|     Divorced|         3298|      15053|                 0|     Y|     Yes|               12|                3|                       4|           80|               2|                7|                    5|              2|             6|                 2|                      0|                   5|\n",
      "| 22|       No|       Non-Travel|     1123|Research & Develo...|              16|        2|       Medical|            1|            22|                      4|  Male|        96|             4|       1|Laboratory Techni...|              4|     Divorced|         2935|       7324|                 1|     Y|     Yes|               13|                3|                       2|           80|               2|                1|                    2|              2|             1|                 0|                      0|                   0|\n",
      "| 53|       No|    Travel_Rarely|     1219|               Sales|               2|        4| Life Sciences|            1|            23|                      1|Female|        78|             2|       4|             Manager|              4|      Married|        15427|      22021|                 2|     Y|      No|               16|                3|                       3|           80|               0|               31|                    3|              3|            25|                 8|                      3|                   7|\n",
      "| 38|       No|    Travel_Rarely|      371|Research & Develo...|               2|        3| Life Sciences|            1|            24|                      4|  Male|        45|             3|       1|  Research Scientist|              4|       Single|         3944|       4306|                 5|     Y|     Yes|               11|                3|                       3|           80|               0|                6|                    3|              3|             3|                 2|                      1|                   2|\n",
      "+---+---------+-----------------+---------+--------------------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+--------------------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "ibm_hr = spark.read.csv(\"../data/WA_Fn-UseC_-HR-Employee-Attrition.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "ibm_hr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+---------+----------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+-------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "|Age|Attrition|BusinessTravel|DailyRate|Department|DistanceFromHome|Education|EducationField|EmployeeCount|EmployeeNumber|EnvironmentSatisfaction|Gender|HourlyRate|JobInvolvement|JobLevel|JobRole|JobSatisfaction|MaritalStatus|MonthlyIncome|MonthlyRate|NumCompaniesWorked|Over18|OverTime|PercentSalaryHike|PerformanceRating|RelationshipSatisfaction|StandardHours|StockOptionLevel|TotalWorkingYears|TrainingTimesLastYear|WorkLifeBalance|YearsAtCompany|YearsInCurrentRole|YearsSinceLastPromotion|YearsWithCurrManager|\n",
      "+---+---------+--------------+---------+----------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+-------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "|  0|        0|             0|        0|         0|               0|        0|             0|            0|             0|                      0|     0|         0|             0|       0|      0|              0|            0|            0|          0|                 0|     0|       0|                0|                0|                       0|            0|               0|                0|                    0|              0|             0|                 0|                      0|                   0|\n",
      "+---+---------+--------------+---------+----------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+-------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if there is any null value\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "ibm_hr.select([count(when(isnan(c), c)).alias(c) for c in ibm_hr.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Attrition|\n",
      "+---------+\n",
      "|      Yes|\n",
      "|       No|\n",
      "|      Yes|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|      Yes|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "|       No|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ibm_hr.select(\"Attrition\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and some viz\n",
    "* generating numerical column for Attrition\n",
    "* plot correlation matrix for some numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|Attrition|Attrition_numerical|\n",
      "+---------+-------------------+\n",
      "|      Yes|                  1|\n",
      "|       No|                  0|\n",
      "|      Yes|                  1|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|      Yes|                  1|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "|       No|                  0|\n",
      "+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "# define function to transform boolean\n",
    "def bool_to_int(b):\n",
    "    if b == \"Yes\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# register user defined function with spark SQL\n",
    "udf_bool_to_int = udf(bool_to_int, IntegerType())\n",
    "# add column\n",
    "ibm_hr_target = ibm_hr.withColumn(\"Attrition_numerical\", udf_bool_to_int(\"Attrition\"))\n",
    "# check the result\n",
    "ibm_hr_target.select(\"Attrition\", \"Attrition_numerical\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+---------+--------------+-----------------------+\n",
      "|Age|DailyRate|DistanceFromHome|Education|EmployeeNumber|EnvironmentSatisfaction|\n",
      "+---+---------+----------------+---------+--------------+-----------------------+\n",
      "| 41|     1102|               1|        2|             1|                      2|\n",
      "| 49|      279|               8|        1|             2|                      3|\n",
      "| 37|     1373|               2|        2|             4|                      4|\n",
      "| 33|     1392|               3|        4|             5|                      4|\n",
      "| 27|      591|               2|        1|             7|                      1|\n",
      "| 32|     1005|               2|        2|             8|                      4|\n",
      "| 59|     1324|               3|        3|            10|                      3|\n",
      "| 30|     1358|              24|        1|            11|                      4|\n",
      "| 38|      216|              23|        3|            12|                      4|\n",
      "| 36|     1299|              27|        3|            13|                      3|\n",
      "| 35|      809|              16|        3|            14|                      1|\n",
      "| 29|      153|              15|        2|            15|                      4|\n",
      "| 31|      670|              26|        1|            16|                      1|\n",
      "| 34|     1346|              19|        2|            18|                      2|\n",
      "| 28|      103|              24|        3|            19|                      3|\n",
      "| 29|     1389|              21|        4|            20|                      2|\n",
      "| 32|      334|               5|        2|            21|                      1|\n",
      "| 22|     1123|              16|        2|            22|                      4|\n",
      "| 53|     1219|               2|        4|            23|                      1|\n",
      "| 38|      371|               2|        3|            24|                      4|\n",
      "+---+---------+----------------+---------+--------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preparation to compute a \"small\" correlation matrix for demostration\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "# the list can be changed on your own preference\n",
    "numerical_list = [u'Age', u'DailyRate', u'DistanceFromHome', u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction']\n",
    "ibm_hr_target_small = ibm_hr_target.select(numerical_list)\n",
    "# cast all string values in df to int\n",
    "ibm_hr_target_small = ibm_hr_target_small.select([col(c).cast(IntegerType()) for c in ibm_hr_target_small.columns])\n",
    "ibm_hr_target_small.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Age: int, DailyRate: int, DistanceFromHome: int, Education: int, EmployeeNumber: int, EnvironmentSatisfaction: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check type\n",
    "ibm_hr_target_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x23ce82cee80>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform spark dataframe to pandas to print the image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(ibm_hr_target_small.select(\"*\").toPandas().corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23ce825c978>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark original way to generate correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010660942645538433"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating single correlation between two columns is also available, however quite difficult to viz by then\n",
    "ibm_hr_target_small.stat.corr(\"Age\", \"DailyRate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Categorical Encoding\n",
    "Prepare data to do machine learning\n",
    "\n",
    "### Get numerical data\n",
    "Actually in a harding way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set list of numerical values\n",
    "numerical = [u'Age', u'DailyRate', u'DistanceFromHome', u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction', \\\n",
    "             u'HourlyRate', u'JobInvolvement', u'JobLevel', u'JobSatisfaction', \\\n",
    "             u'MonthlyIncome', u'MonthlyRate', u'NumCompaniesWorked', \\\n",
    "             u'PercentSalaryHike', u'PerformanceRating', u'RelationshipSatisfaction', \\\n",
    "             u'StockOptionLevel', u'TotalWorkingYears', \\\n",
    "             u'TrainingTimesLastYear', u'WorkLifeBalance', u'YearsAtCompany', \\\n",
    "             u'YearsInCurrentRole', u'YearsSinceLastPromotion', u'YearsWithCurrManager']\n",
    "# drop numerical columns\n",
    "ibm_hr_no_numerical = ibm_hr_target.drop(*numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get categorical data\n",
    "Different from the approach in the [Kaggle Kernel](https://www.kaggle.com/arthurtok/employee-attrition-via-rf-gbm), I need to select the categorical in a quite hard coding way as spark read all data type as **String**.\n",
    "\n",
    "I manuly define those column with values **COUNT DISINCT** smaller than 20 as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Attrition: string, BusinessTravel: string, Department: string, EducationField: string, EmployeeCount: string, Gender: string, JobRole: string, MaritalStatus: string, Over18: string, OverTime: string, StandardHours: string, Attrition_numerical: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of the columns\n",
    "ibm_hr_no_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition :2\n",
      "BusinessTravel :3\n",
      "Department :3\n",
      "EducationField :6\n",
      "EmployeeCount :1\n",
      "Gender :2\n",
      "JobRole :9\n",
      "MaritalStatus :3\n",
      "Over18 :1\n",
      "OverTime :2\n",
      "StandardHours :1\n",
      "Attrition_numerical :2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Attrition',\n",
       " 'BusinessTravel',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'MaritalStatus',\n",
       " 'OverTime',\n",
       " 'Attrition_numerical']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a list to store columns with categorical data type\n",
    "categorical = []\n",
    "for col in ibm_hr_no_numerical.columns:\n",
    "    num_types = ibm_hr_no_numerical.select(col).distinct().count()\n",
    "    print(col + \" :\" + str(num_types))\n",
    "    if num_types < 20 and num_types != 1:\n",
    "        categorical.append(col)\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessTravel',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'MaritalStatus',\n",
       " 'OverTime']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove our target; note that we must use deep copy to ensure the originality of lists\n",
    "import copy\n",
    "categorical_no_target = copy.deepcopy(categorical)\n",
    "categorical_no_target.remove(\"Attrition_numerical\")\n",
    "categorical_no_target.remove(\"Attrition\")\n",
    "categorical_no_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------+------+--------------------+-------------+--------+\n",
      "|   BusinessTravel|          Department|EducationField|Gender|             JobRole|MaritalStatus|OverTime|\n",
      "+-----------------+--------------------+--------------+------+--------------------+-------------+--------+\n",
      "|    Travel_Rarely|               Sales| Life Sciences|Female|     Sales Executive|       Single|     Yes|\n",
      "|Travel_Frequently|Research & Develo...| Life Sciences|  Male|  Research Scientist|      Married|      No|\n",
      "|    Travel_Rarely|Research & Develo...|         Other|  Male|Laboratory Techni...|       Single|     Yes|\n",
      "|Travel_Frequently|Research & Develo...| Life Sciences|Female|  Research Scientist|      Married|     Yes|\n",
      "|    Travel_Rarely|Research & Develo...|       Medical|  Male|Laboratory Techni...|      Married|      No|\n",
      "|Travel_Frequently|Research & Develo...| Life Sciences|  Male|Laboratory Techni...|       Single|      No|\n",
      "|    Travel_Rarely|Research & Develo...|       Medical|Female|Laboratory Techni...|      Married|     Yes|\n",
      "|    Travel_Rarely|Research & Develo...| Life Sciences|  Male|Laboratory Techni...|     Divorced|      No|\n",
      "|Travel_Frequently|Research & Develo...| Life Sciences|  Male|Manufacturing Dir...|       Single|      No|\n",
      "|    Travel_Rarely|Research & Develo...|       Medical|  Male|Healthcare Repres...|      Married|      No|\n",
      "|    Travel_Rarely|Research & Develo...|       Medical|  Male|Laboratory Techni...|      Married|      No|\n",
      "|    Travel_Rarely|Research & Develo...| Life Sciences|Female|Laboratory Techni...|       Single|     Yes|\n",
      "|    Travel_Rarely|Research & Develo...| Life Sciences|  Male|  Research Scientist|     Divorced|      No|\n",
      "|    Travel_Rarely|Research & Develo...|       Medical|  Male|Laboratory Techni...|     Divorced|      No|\n",
      "|    Travel_Rarely|Research & Develo...| Life Sciences|  Male|Laboratory Techni...|       Single|     Yes|\n",
      "|    Travel_Rarely|Research & Develo...| Life Sciences|Female|Manufacturing Dir...|     Divorced|      No|\n",
      "|    Travel_Rarely|Research & Develo...| Life Sciences|  Male|  Research Scientist|     Divorced|     Yes|\n",
      "|       Non-Travel|Research & Develo...|       Medical|  Male|Laboratory Techni...|     Divorced|     Yes|\n",
      "|    Travel_Rarely|               Sales| Life Sciences|Female|             Manager|      Married|      No|\n",
      "|    Travel_Rarely|Research & Develo...| Life Sciences|  Male|  Research Scientist|       Single|     Yes|\n",
      "+-----------------+--------------------+--------------+------+--------------------+-------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get categorical dataframe\n",
    "ibm_hr_cat = ibm_hr_no_numerical.select(categorical_no_target)\n",
    "ibm_hr_cat.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to get dummny in a spark way\n",
    "Reference can be found at [here](https://stackoverflow.com/questions/42805663/e-num-get-dummies-in-pyspark).\n",
    "\n",
    "Although the program produces the desired result, it takes considerable amount of time, which discourages me from reproducing this on my dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "| ID|TYPE|CODE|e_TYPE_B|e_TYPE_C|e_TYPE_A|e_CODE_X1|e_CODE_X3|e_CODE_X2|\n",
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "|  1|   A|  X1|       0|       0|       1|        1|        0|        0|\n",
      "|  2|   B|  X2|       1|       0|       0|        0|        0|        1|\n",
      "|  3|   B|  X3|       1|       0|       0|        0|        1|        0|\n",
      "|  1|   B|  X3|       1|       0|       0|        0|        1|        0|\n",
      "|  2|   C|  X2|       0|       1|       0|        0|        0|        1|\n",
      "|  3|   C|  X2|       0|       1|       0|        0|        0|        1|\n",
      "|  1|   C|  X1|       0|       1|       0|        1|        0|        0|\n",
      "|  1|   B|  X1|       1|       0|       0|        1|        0|        0|\n",
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "import pyspark.sql.functions as F\n",
    "df = sqlContext.createDataFrame([\n",
    "    (1, \"A\", \"X1\"),\n",
    "    (2, \"B\", \"X2\"),\n",
    "    (3, \"B\", \"X3\"),\n",
    "    (1, \"B\", \"X3\"),\n",
    "    (2, \"C\", \"X2\"),\n",
    "    (3, \"C\", \"X2\"),\n",
    "    (1, \"C\", \"X1\"),\n",
    "    (1, \"B\", \"X1\"),\n",
    "], [\"ID\", \"TYPE\", \"CODE\"])\n",
    "\n",
    "types = df.select(\"TYPE\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "codes = df.select(\"CODE\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "types_expr = [F.when(F.col(\"TYPE\") == ty, 1).otherwise(0).alias(\"e_TYPE_\" + ty) for ty in types]\n",
    "codes_expr = [F.when(F.col(\"CODE\") == code, 1).otherwise(0).alias(\"e_CODE_\" + code) for code in codes]\n",
    "df = df.select(\"ID\", \"TYPE\", \"CODE\", *types_expr+codes_expr)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I turned to **Pandas** for help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BusinessTravel_Non-Travel</th>\n",
       "      <th>BusinessTravel_Travel_Frequently</th>\n",
       "      <th>BusinessTravel_Travel_Rarely</th>\n",
       "      <th>Department_Human Resources</th>\n",
       "      <th>Department_Research &amp; Development</th>\n",
       "      <th>Department_Sales</th>\n",
       "      <th>EducationField_Human Resources</th>\n",
       "      <th>EducationField_Life Sciences</th>\n",
       "      <th>EducationField_Marketing</th>\n",
       "      <th>EducationField_Medical</th>\n",
       "      <th>...</th>\n",
       "      <th>JobRole_Manufacturing Director</th>\n",
       "      <th>JobRole_Research Director</th>\n",
       "      <th>JobRole_Research Scientist</th>\n",
       "      <th>JobRole_Sales Executive</th>\n",
       "      <th>JobRole_Sales Representative</th>\n",
       "      <th>MaritalStatus_Divorced</th>\n",
       "      <th>MaritalStatus_Married</th>\n",
       "      <th>MaritalStatus_Single</th>\n",
       "      <th>OverTime_No</th>\n",
       "      <th>OverTime_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BusinessTravel_Non-Travel  BusinessTravel_Travel_Frequently  \\\n",
       "0                          0                                 0   \n",
       "1                          0                                 1   \n",
       "2                          0                                 0   \n",
       "\n",
       "   BusinessTravel_Travel_Rarely  Department_Human Resources  \\\n",
       "0                             1                           0   \n",
       "1                             0                           0   \n",
       "2                             1                           0   \n",
       "\n",
       "   Department_Research & Development  Department_Sales  \\\n",
       "0                                  0                 1   \n",
       "1                                  1                 0   \n",
       "2                                  1                 0   \n",
       "\n",
       "   EducationField_Human Resources  EducationField_Life Sciences  \\\n",
       "0                               0                             1   \n",
       "1                               0                             1   \n",
       "2                               0                             0   \n",
       "\n",
       "   EducationField_Marketing  EducationField_Medical      ...       \\\n",
       "0                         0                       0      ...        \n",
       "1                         0                       0      ...        \n",
       "2                         0                       0      ...        \n",
       "\n",
       "   JobRole_Manufacturing Director  JobRole_Research Director  \\\n",
       "0                               0                          0   \n",
       "1                               0                          0   \n",
       "2                               0                          0   \n",
       "\n",
       "   JobRole_Research Scientist  JobRole_Sales Executive  \\\n",
       "0                           0                        1   \n",
       "1                           1                        0   \n",
       "2                           0                        0   \n",
       "\n",
       "   JobRole_Sales Representative  MaritalStatus_Divorced  \\\n",
       "0                             0                       0   \n",
       "1                             0                       0   \n",
       "2                             0                       0   \n",
       "\n",
       "   MaritalStatus_Married  MaritalStatus_Single  OverTime_No  OverTime_Yes  \n",
       "0                      0                     1            0             1  \n",
       "1                      1                     0            1             0  \n",
       "2                      0                     1            0             1  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pandas to get_dummies\n",
    "import pandas as pd\n",
    "pd_cat = pd.get_dummies(ibm_hr_cat.select(\"*\").toPandas())\n",
    "pd_cat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------------------+----------------------------+--------------------------+---------------------------------+----------------+------------------------------+----------------------------+------------------------+----------------------+--------------------+-------------------------------+-------------+-----------+---------------------------------+-----------------------+-----------------------------+---------------+------------------------------+-------------------------+--------------------------+-----------------------+----------------------------+----------------------+---------------------+--------------------+-----------+------------+\n",
      "|BusinessTravel_Non-Travel|BusinessTravel_Travel_Frequently|BusinessTravel_Travel_Rarely|Department_Human Resources|Department_Research & Development|Department_Sales|EducationField_Human Resources|EducationField_Life Sciences|EducationField_Marketing|EducationField_Medical|EducationField_Other|EducationField_Technical Degree|Gender_Female|Gender_Male|JobRole_Healthcare Representative|JobRole_Human Resources|JobRole_Laboratory Technician|JobRole_Manager|JobRole_Manufacturing Director|JobRole_Research Director|JobRole_Research Scientist|JobRole_Sales Executive|JobRole_Sales Representative|MaritalStatus_Divorced|MaritalStatus_Married|MaritalStatus_Single|OverTime_No|OverTime_Yes|\n",
      "+-------------------------+--------------------------------+----------------------------+--------------------------+---------------------------------+----------------+------------------------------+----------------------------+------------------------+----------------------+--------------------+-------------------------------+-------------+-----------+---------------------------------+-----------------------+-----------------------------+---------------+------------------------------+-------------------------+--------------------------+-----------------------+----------------------------+----------------------+---------------------+--------------------+-----------+------------+\n",
      "|                        0|                               0|                           1|                         0|                                0|               1|                             0|                           1|                       0|                     0|                   0|                              0|            1|          0|                                0|                      0|                            0|              0|                             0|                        0|                         0|                      1|                           0|                     0|                    0|                   1|          0|           1|\n",
      "|                        0|                               1|                           0|                         0|                                1|               0|                             0|                           1|                       0|                     0|                   0|                              0|            0|          1|                                0|                      0|                            0|              0|                             0|                        0|                         1|                      0|                           0|                     0|                    1|                   0|          1|           0|\n",
      "|                        0|                               0|                           1|                         0|                                1|               0|                             0|                           0|                       0|                     0|                   1|                              0|            0|          1|                                0|                      0|                            1|              0|                             0|                        0|                         0|                      0|                           0|                     0|                    0|                   1|          0|           1|\n",
      "+-------------------------+--------------------------------+----------------------------+--------------------------+---------------------------------+----------------+------------------------------+----------------------------+------------------------+----------------------+--------------------+-------------------------------+-------------+-----------+---------------------------------+-----------------------+-----------------------------+---------------+------------------------------+-------------------------+--------------------------+-----------------------+----------------------------+----------------------+---------------------+--------------------+-----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert back to spark dataframe\n",
    "ibm_hr_cat_dum = spark.createDataFrame(pd_cat)\n",
    "ibm_hr_cat_dum.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the dataframe\n",
    "This is where we are going to train and test our data on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+---------+--------------+-----------------------+----------+--------------+--------+---------------+-------------+-----------+------------------+-----------------+-----------------+------------------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "|Age|DailyRate|DistanceFromHome|Education|EmployeeNumber|EnvironmentSatisfaction|HourlyRate|JobInvolvement|JobLevel|JobSatisfaction|MonthlyIncome|MonthlyRate|NumCompaniesWorked|PercentSalaryHike|PerformanceRating|RelationshipSatisfaction|StockOptionLevel|TotalWorkingYears|TrainingTimesLastYear|WorkLifeBalance|YearsAtCompany|YearsInCurrentRole|YearsSinceLastPromotion|YearsWithCurrManager|\n",
      "+---+---------+----------------+---------+--------------+-----------------------+----------+--------------+--------+---------------+-------------+-----------+------------------+-----------------+-----------------+------------------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "| 41|     1102|               1|        2|             1|                      2|        94|             3|       2|              4|         5993|      19479|                 8|               11|                3|                       1|               0|                8|                    0|              1|             6|                 4|                      0|                   5|\n",
      "| 49|      279|               8|        1|             2|                      3|        61|             2|       2|              2|         5130|      24907|                 1|               23|                4|                       4|               1|               10|                    3|              3|            10|                 7|                      1|                   7|\n",
      "| 37|     1373|               2|        2|             4|                      4|        92|             2|       1|              3|         2090|       2396|                 6|               15|                3|                       2|               0|                7|                    3|              3|             0|                 0|                      0|                   0|\n",
      "+---+---------+----------------+---------+--------------+-----------------------+----------+--------------+--------+---------------+-------------+-----------+------------------+-----------------+-----------------+------------------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get numerical features\n",
    "ibm_hr_int = ibm_hr_target.select(numerical)\n",
    "ibm_hr_int.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- DailyRate: integer (nullable = true)\n",
      " |-- DistanceFromHome: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- EmployeeNumber: integer (nullable = true)\n",
      " |-- EnvironmentSatisfaction: integer (nullable = true)\n",
      " |-- HourlyRate: integer (nullable = true)\n",
      " |-- JobInvolvement: integer (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- MonthlyIncome: integer (nullable = true)\n",
      " |-- MonthlyRate: integer (nullable = true)\n",
      " |-- NumCompaniesWorked: integer (nullable = true)\n",
      " |-- PercentSalaryHike: integer (nullable = true)\n",
      " |-- PerformanceRating: integer (nullable = true)\n",
      " |-- RelationshipSatisfaction: integer (nullable = true)\n",
      " |-- StockOptionLevel: integer (nullable = true)\n",
      " |-- TotalWorkingYears: integer (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: integer (nullable = true)\n",
      " |-- YearsAtCompany: integer (nullable = true)\n",
      " |-- YearsInCurrentRole: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- YearsWithCurrManager: integer (nullable = true)\n",
      " |-- BusinessTravel_Non-Travel: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Frequently: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Rarely: integer (nullable = true)\n",
      " |-- Department_Human Resources: integer (nullable = true)\n",
      " |-- Department_Research & Development: integer (nullable = true)\n",
      " |-- Department_Sales: integer (nullable = true)\n",
      " |-- EducationField_Human Resources: integer (nullable = true)\n",
      " |-- EducationField_Life Sciences: integer (nullable = true)\n",
      " |-- EducationField_Marketing: integer (nullable = true)\n",
      " |-- EducationField_Medical: integer (nullable = true)\n",
      " |-- EducationField_Other: integer (nullable = true)\n",
      " |-- EducationField_Technical Degree: integer (nullable = true)\n",
      " |-- Gender_Female: integer (nullable = true)\n",
      " |-- Gender_Male: integer (nullable = true)\n",
      " |-- JobRole_Healthcare Representative: integer (nullable = true)\n",
      " |-- JobRole_Human Resources: integer (nullable = true)\n",
      " |-- JobRole_Laboratory Technician: integer (nullable = true)\n",
      " |-- JobRole_Manager: integer (nullable = true)\n",
      " |-- JobRole_Manufacturing Director: integer (nullable = true)\n",
      " |-- JobRole_Research Director: integer (nullable = true)\n",
      " |-- JobRole_Research Scientist: integer (nullable = true)\n",
      " |-- JobRole_Sales Executive: integer (nullable = true)\n",
      " |-- JobRole_Sales Representative: integer (nullable = true)\n",
      " |-- MaritalStatus_Divorced: integer (nullable = true)\n",
      " |-- MaritalStatus_Married: integer (nullable = true)\n",
      " |-- MaritalStatus_Single: integer (nullable = true)\n",
      " |-- OverTime_No: integer (nullable = true)\n",
      " |-- OverTime_Yes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concat the two dataframes together columnwise\n",
    "ibm_hr_final = ibm_hr_int.join(ibm_hr_cat_dum)\n",
    "for c in ibm_hr_final.columns:\n",
    "    ibm_hr_final = ibm_hr_final.withColumn(c, ibm_hr_final[c].cast(IntegerType()))\n",
    "ibm_hr_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, plot is also difficult with *pyspark* in *jupyter notebook*.\n",
    "\n",
    "Using *Zeppelin* might have a better result, but she does not works well with *Windows*.\n",
    "\n",
    "So at here, I only do a brief stat on our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regarding attrition:\n",
      "Yes: 237\n",
      "No: 1233\n"
     ]
    }
   ],
   "source": [
    "print(\"regarding attrition:\")\n",
    "print(\"Yes: \" + str(ibm_hr.filter(ibm_hr['Attrition'] == \"Yes\").count()))\n",
    "print(\"No: \" + str(ibm_hr.filter(ibm_hr['Attrition'] == \"No\").count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial for user self-defined get dummny function\n",
    "Reference can be found at [here](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf).\n",
    "\n",
    "This should work with *pyspark*; however, the error is currently beyond my level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- DailyRate: integer (nullable = true)\n",
      " |-- DistanceFromHome: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- EmployeeNumber: integer (nullable = true)\n",
      " |-- EnvironmentSatisfaction: integer (nullable = true)\n",
      " |-- HourlyRate: integer (nullable = true)\n",
      " |-- JobInvolvement: integer (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- MonthlyIncome: integer (nullable = true)\n",
      " |-- MonthlyRate: integer (nullable = true)\n",
      " |-- NumCompaniesWorked: integer (nullable = true)\n",
      " |-- PercentSalaryHike: integer (nullable = true)\n",
      " |-- PerformanceRating: integer (nullable = true)\n",
      " |-- RelationshipSatisfaction: integer (nullable = true)\n",
      " |-- StockOptionLevel: integer (nullable = true)\n",
      " |-- TotalWorkingYears: integer (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: integer (nullable = true)\n",
      " |-- YearsAtCompany: integer (nullable = true)\n",
      " |-- YearsInCurrentRole: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- YearsWithCurrManager: integer (nullable = true)\n",
      " |-- BusinessTravel: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- EducationField: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- JobRole: string (nullable = true)\n",
      " |-- MaritalStatus: string (nullable = true)\n",
      " |-- OverTime: string (nullable = true)\n",
      " |-- Attrition: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concat the two dataframes together columnwise\n",
    "ibm_hr_final2 = ibm_hr_final.join(ibm_hr.select(\"Attrition\"))\n",
    "ibm_hr_final2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "def get_dummy(df, categoricalCols, continuousCols, labelCol):\n",
    "    indexers = [StringIndexer(inputCol = c, outputCol = \"{0}_indexed\".format(c)) for c in categoricalCols]\n",
    "    # default setting: dropLast = True\n",
    "    encoders = [OneHotEncoder(inputCol = indexer.getOutputCol(), \\\n",
    "                outputCol = \"{0}_encoded\".format(indexer.getOutputCol())) \\\n",
    "                for indexer in indexers]\n",
    "    assembler = VectorAssembler(inputCols = [encoder.getOutputCol() for encoder in encoders] \\\n",
    "                + continuousCols, outputCol = \"features\")\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "    data = data.withColumn('label', col(labelCol))\n",
    "    return data.select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessTravel',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'MaritalStatus',\n",
       " 'OverTime']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_no_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'DailyRate',\n",
       " 'DistanceFromHome',\n",
       " 'Education',\n",
       " 'EmployeeNumber',\n",
       " 'EnvironmentSatisfaction',\n",
       " 'HourlyRate',\n",
       " 'JobInvolvement',\n",
       " 'JobLevel',\n",
       " 'JobSatisfaction',\n",
       " 'MonthlyIncome',\n",
       " 'MonthlyRate',\n",
       " 'NumCompaniesWorked',\n",
       " 'PercentSalaryHike',\n",
       " 'PerformanceRating',\n",
       " 'RelationshipSatisfaction',\n",
       " 'StockOptionLevel',\n",
       " 'TotalWorkingYears',\n",
       " 'TrainingTimesLastYear',\n",
       " 'WorkLifeBalance',\n",
       " 'YearsAtCompany',\n",
       " 'YearsInCurrentRole',\n",
       " 'YearsSinceLastPromotion',\n",
       " 'YearsWithCurrManager']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attrition'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = \"Attrition\"\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner\\n:- Project\\n:  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n+- Project [BusinessTravel#12]\\n   +- Filter AtLeastNNulls(n, BusinessTravel#12)\\n      +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nand\\nProject\\n+- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1537.fit.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nJoin Inner\n:- Project\n:  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\n+- Project [BusinessTravel#12]\n   +- Filter AtLeastNNulls(n, BusinessTravel#12)\n      +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\nand\nProject\n+- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1124)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1121)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1121)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1103)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\r\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\r\n\tat org.apache.spark.sql.Dataset.rdd$lzycompute(Dataset.scala:2975)\r\n\tat org.apache.spark.sql.Dataset.rdd(Dataset.scala:2973)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:138)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:109)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-20c9d4823668>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dummy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mibm_hr_final2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_no_target\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mnumerical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-94-95f67489b480>\u001b[0m in \u001b[0;36mget_dummy\u001b[1;34m(df, categoricalCols, continuousCols, labelCol)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0massembler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoders\u001b[0m\u001b[1;33m]\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[0mcontinuousCols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexers\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mencoders\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0massembler\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \"\"\"\n\u001b[0;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner\\n:- Project\\n:  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n+- Project [BusinessTravel#12]\\n   +- Filter AtLeastNNulls(n, BusinessTravel#12)\\n      +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nand\\nProject\\n+- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
     ]
    }
   ],
   "source": [
    "data = get_dummy(ibm_hr_final2, categorical_no_target ,numerical, label)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
