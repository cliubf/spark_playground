{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# local spark setup with ipython\n",
    "\n",
    "## prerequisite：\n",
    "\n",
    "> ipython in your local PC, [anaconda](https://www.anaconda.com/download/) recommended\n",
    "\n",
    "> spark in your local PC, extracted to directory. [Download page](https://spark.apache.org/downloads.html)\n",
    "\n",
    "> Powershell (For Windows)\n",
    "\n",
    "## command to run\n",
    "\n",
    "1. initialize ipython from your cmd\n",
    "```bat\n",
    "jupyter notebook\n",
    "```\n",
    "\n",
    "2. install packages if required (findspark for example)\n",
    "```bat\n",
    "pip install findspark\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important: please modify the path as your local spark location \n",
    "spark_path = \"D:/spark-2.3.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark is well set at 2018-07-31 09:51:50.940104\n"
     ]
    }
   ],
   "source": [
    "# configure spark, a message regrads time will print out if success\n",
    "import findspark\n",
    "from datetime import datetime\n",
    "findspark.init(\"D:/spark-2.3.1-bin-hadoop2.7\")\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ibm_hr\")\n",
    "sc = SparkContext(conf = conf)\n",
    "print(\"spark is well set at \" + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Chengzhong:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ibm_hr</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x299c615e6a0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open spark session\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "\n",
    "## Disclaimer: \n",
    "* for academic use only\n",
    "* data is available on [Kaggle](https://www.kaggle.com/pavansubhasht/ibm-hr-analytics-attrition-dataset/kernels)\n",
    "* most of the analysis pipeline in this notebook is referenced from this [kernel](https://www.kaggle.com/arthurtok/employee-attrition-via-rf-gbm). I do not own the copyright. For use other than academic, please contact with original author.\n",
    "\n",
    "### Data loading and quality check\n",
    "* findout if data is loaded correctly\n",
    "* check if there is any null in any columns\n",
    "* Get familar with our target: Attrition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------------+---------+--------------------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+--------------------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "|Age|Attrition|   BusinessTravel|DailyRate|          Department|DistanceFromHome|Education|EducationField|EmployeeCount|EmployeeNumber|EnvironmentSatisfaction|Gender|HourlyRate|JobInvolvement|JobLevel|             JobRole|JobSatisfaction|MaritalStatus|MonthlyIncome|MonthlyRate|NumCompaniesWorked|Over18|OverTime|PercentSalaryHike|PerformanceRating|RelationshipSatisfaction|StandardHours|StockOptionLevel|TotalWorkingYears|TrainingTimesLastYear|WorkLifeBalance|YearsAtCompany|YearsInCurrentRole|YearsSinceLastPromotion|YearsWithCurrManager|\n",
      "+---+---------+-----------------+---------+--------------------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+--------------------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "| 41|      Yes|    Travel_Rarely|     1102|               Sales|               1|        2| Life Sciences|            1|             1|                      2|Female|        94|             3|       2|     Sales Executive|              4|       Single|         5993|      19479|                 8|     Y|     Yes|               11|                3|                       1|           80|               0|                8|                    0|              1|             6|                 4|                      0|                   5|\n",
      "| 49|       No|Travel_Frequently|      279|Research & Develo...|               8|        1| Life Sciences|            1|             2|                      3|  Male|        61|             2|       2|  Research Scientist|              2|      Married|         5130|      24907|                 1|     Y|      No|               23|                4|                       4|           80|               1|               10|                    3|              3|            10|                 7|                      1|                   7|\n",
      "| 37|      Yes|    Travel_Rarely|     1373|Research & Develo...|               2|        2|         Other|            1|             4|                      4|  Male|        92|             2|       1|Laboratory Techni...|              3|       Single|         2090|       2396|                 6|     Y|     Yes|               15|                3|                       2|           80|               0|                7|                    3|              3|             0|                 0|                      0|                   0|\n",
      "+---+---------+-----------------+---------+--------------------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+--------------------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "ibm_hr = spark.read.csv(\"../data/WA_Fn-UseC_-HR-Employee-Attrition.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "ibm_hr.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+--------------+---------+----------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+-------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "|Age|Attrition|BusinessTravel|DailyRate|Department|DistanceFromHome|Education|EducationField|EmployeeCount|EmployeeNumber|EnvironmentSatisfaction|Gender|HourlyRate|JobInvolvement|JobLevel|JobRole|JobSatisfaction|MaritalStatus|MonthlyIncome|MonthlyRate|NumCompaniesWorked|Over18|OverTime|PercentSalaryHike|PerformanceRating|RelationshipSatisfaction|StandardHours|StockOptionLevel|TotalWorkingYears|TrainingTimesLastYear|WorkLifeBalance|YearsAtCompany|YearsInCurrentRole|YearsSinceLastPromotion|YearsWithCurrManager|\n",
      "+---+---------+--------------+---------+----------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+-------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "|  0|        0|             0|        0|         0|               0|        0|             0|            0|             0|                      0|     0|         0|             0|       0|      0|              0|            0|            0|          0|                 0|     0|       0|                0|                0|                       0|            0|               0|                0|                    0|              0|             0|                 0|                      0|                   0|\n",
      "+---+---------+--------------+---------+----------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+-------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check if there is any null value\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "ibm_hr.select([count(when(isnan(c), c)).alias(c) for c in ibm_hr.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|Attrition|\n",
      "+---------+\n",
      "|      Yes|\n",
      "|       No|\n",
      "|      Yes|\n",
      "+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ibm_hr.select(\"Attrition\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform and some viz\n",
    "* generating numerical column for Attrition\n",
    "* plot correlation matrix for some numerical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|Attrition|Attrition_numerical|\n",
      "+---------+-------------------+\n",
      "|      Yes|                  1|\n",
      "|       No|                  0|\n",
      "|      Yes|                  1|\n",
      "+---------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "# define function to transform boolean\n",
    "def bool_to_int(b):\n",
    "    if b == \"Yes\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# register user defined function with spark SQL\n",
    "udf_bool_to_int = udf(bool_to_int, IntegerType())\n",
    "# add column\n",
    "ibm_hr_target = ibm_hr.withColumn(\"Attrition_numerical\", udf_bool_to_int(\"Attrition\"))\n",
    "# check the result\n",
    "ibm_hr_target.select(\"Attrition\", \"Attrition_numerical\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+---------+--------------+-----------------------+\n",
      "|Age|DailyRate|DistanceFromHome|Education|EmployeeNumber|EnvironmentSatisfaction|\n",
      "+---+---------+----------------+---------+--------------+-----------------------+\n",
      "| 41|     1102|               1|        2|             1|                      2|\n",
      "| 49|      279|               8|        1|             2|                      3|\n",
      "| 37|     1373|               2|        2|             4|                      4|\n",
      "| 33|     1392|               3|        4|             5|                      4|\n",
      "| 27|      591|               2|        1|             7|                      1|\n",
      "| 32|     1005|               2|        2|             8|                      4|\n",
      "| 59|     1324|               3|        3|            10|                      3|\n",
      "| 30|     1358|              24|        1|            11|                      4|\n",
      "| 38|      216|              23|        3|            12|                      4|\n",
      "| 36|     1299|              27|        3|            13|                      3|\n",
      "| 35|      809|              16|        3|            14|                      1|\n",
      "| 29|      153|              15|        2|            15|                      4|\n",
      "| 31|      670|              26|        1|            16|                      1|\n",
      "| 34|     1346|              19|        2|            18|                      2|\n",
      "| 28|      103|              24|        3|            19|                      3|\n",
      "| 29|     1389|              21|        4|            20|                      2|\n",
      "| 32|      334|               5|        2|            21|                      1|\n",
      "| 22|     1123|              16|        2|            22|                      4|\n",
      "| 53|     1219|               2|        4|            23|                      1|\n",
      "| 38|      371|               2|        3|            24|                      4|\n",
      "+---+---------+----------------+---------+--------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preparation to compute a \"small\" correlation matrix for demostration\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "# the list can be changed on your own preference\n",
    "numerical_list = [u'Age', u'DailyRate', u'DistanceFromHome', u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction']\n",
    "ibm_hr_target_small = ibm_hr_target.select(numerical_list)\n",
    "# cast all string values in df to int\n",
    "ibm_hr_target_small = ibm_hr_target_small.select([col(c).cast(IntegerType()) for c in ibm_hr_target_small.columns])\n",
    "ibm_hr_target_small.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Age: int, DailyRate: int, DistanceFromHome: int, Education: int, EmployeeNumber: int, EnvironmentSatisfaction: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check type\n",
    "ibm_hr_target_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x299c83da0f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform spark dataframe to pandas to print the image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.matshow(ibm_hr_target_small.select(\"*\").toPandas().corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAECCAYAAADesWqHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x299c835def0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spark original way to generate correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.010660942645538433"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating single correlation between two columns is also available, however quite difficult to viz by then\n",
    "ibm_hr_target_small.stat.corr(\"Age\", \"DailyRate\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering & Categorical Encoding\n",
    "Prepare data to do machine learning\n",
    "\n",
    "### Get numerical data\n",
    "Actually in a harding way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set list of numerical values\n",
    "numerical = [u'Age', u'DailyRate', u'DistanceFromHome', u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction', \\\n",
    "             u'HourlyRate', u'JobInvolvement', u'JobLevel', u'JobSatisfaction', \\\n",
    "             u'MonthlyIncome', u'MonthlyRate', u'NumCompaniesWorked', \\\n",
    "             u'PercentSalaryHike', u'PerformanceRating', u'RelationshipSatisfaction', \\\n",
    "             u'StockOptionLevel', u'TotalWorkingYears', \\\n",
    "             u'TrainingTimesLastYear', u'WorkLifeBalance', u'YearsAtCompany', \\\n",
    "             u'YearsInCurrentRole', u'YearsSinceLastPromotion', u'YearsWithCurrManager']\n",
    "# drop numerical columns\n",
    "ibm_hr_no_numerical = ibm_hr_target.drop(*numerical)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get categorical data\n",
    "Different from the approach in the [Kaggle Kernel](https://www.kaggle.com/arthurtok/employee-attrition-via-rf-gbm), I need to select the categorical in a quite hard coding way as spark read all data type as **String**.\n",
    "\n",
    "I manuly define those column with values **COUNT DISINCT** smaller than 20 as categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Attrition: string, BusinessTravel: string, Department: string, EducationField: string, EmployeeCount: string, Gender: string, JobRole: string, MaritalStatus: string, Over18: string, OverTime: string, StandardHours: string, Attrition_numerical: int]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the type of the columns\n",
    "ibm_hr_no_numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attrition :2\n",
      "BusinessTravel :3\n",
      "Department :3\n",
      "EducationField :6\n",
      "EmployeeCount :1\n",
      "Gender :2\n",
      "JobRole :9\n",
      "MaritalStatus :3\n",
      "Over18 :1\n",
      "OverTime :2\n",
      "StandardHours :1\n",
      "Attrition_numerical :2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Attrition',\n",
       " 'BusinessTravel',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'MaritalStatus',\n",
       " 'OverTime',\n",
       " 'Attrition_numerical']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set a list to store columns with categorical data type\n",
    "categorical = []\n",
    "for col in ibm_hr_no_numerical.columns:\n",
    "    num_types = ibm_hr_no_numerical.select(col).distinct().count()\n",
    "    print(col + \" :\" + str(num_types))\n",
    "    if num_types < 20 and num_types != 1:\n",
    "        categorical.append(col)\n",
    "categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessTravel',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'MaritalStatus',\n",
       " 'OverTime']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove our target; note that we must use deep copy to ensure the originality of lists\n",
    "import copy\n",
    "categorical_no_target = copy.deepcopy(categorical)\n",
    "categorical_no_target.remove(\"Attrition_numerical\")\n",
    "categorical_no_target.remove(\"Attrition\")\n",
    "categorical_no_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+--------------+------+--------------------+-------------+--------+\n",
      "|   BusinessTravel|          Department|EducationField|Gender|             JobRole|MaritalStatus|OverTime|\n",
      "+-----------------+--------------------+--------------+------+--------------------+-------------+--------+\n",
      "|    Travel_Rarely|               Sales| Life Sciences|Female|     Sales Executive|       Single|     Yes|\n",
      "|Travel_Frequently|Research & Develo...| Life Sciences|  Male|  Research Scientist|      Married|      No|\n",
      "|    Travel_Rarely|Research & Develo...|         Other|  Male|Laboratory Techni...|       Single|     Yes|\n",
      "+-----------------+--------------------+--------------+------+--------------------+-------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get categorical dataframe\n",
    "ibm_hr_cat = ibm_hr_no_numerical.select(categorical_no_target)\n",
    "ibm_hr_cat.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt to get dummny in a spark way\n",
    "Reference can be found at [here](https://stackoverflow.com/questions/42805663/e-num-get-dummies-in-pyspark).\n",
    "\n",
    "Although the program produces the desired result, it takes considerable amount of time, which discourages me from reproducing this on my dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "| ID|TYPE|CODE|e_TYPE_B|e_TYPE_C|e_TYPE_A|e_CODE_X1|e_CODE_X3|e_CODE_X2|\n",
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "|  1|   A|  X1|       0|       0|       1|        1|        0|        0|\n",
      "|  2|   B|  X2|       1|       0|       0|        0|        0|        1|\n",
      "|  3|   B|  X3|       1|       0|       0|        0|        1|        0|\n",
      "|  1|   B|  X3|       1|       0|       0|        0|        1|        0|\n",
      "|  2|   C|  X2|       0|       1|       0|        0|        0|        1|\n",
      "|  3|   C|  X2|       0|       1|       0|        0|        0|        1|\n",
      "|  1|   C|  X1|       0|       1|       0|        1|        0|        0|\n",
      "|  1|   B|  X1|       1|       0|       0|        1|        0|        0|\n",
      "+---+----+----+--------+--------+--------+---------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "import pyspark.sql.functions as F\n",
    "df = sqlContext.createDataFrame([\n",
    "    (1, \"A\", \"X1\"),\n",
    "    (2, \"B\", \"X2\"),\n",
    "    (3, \"B\", \"X3\"),\n",
    "    (1, \"B\", \"X3\"),\n",
    "    (2, \"C\", \"X2\"),\n",
    "    (3, \"C\", \"X2\"),\n",
    "    (1, \"C\", \"X1\"),\n",
    "    (1, \"B\", \"X1\"),\n",
    "], [\"ID\", \"TYPE\", \"CODE\"])\n",
    "\n",
    "types = df.select(\"TYPE\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "codes = df.select(\"CODE\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "types_expr = [F.when(F.col(\"TYPE\") == ty, 1).otherwise(0).alias(\"e_TYPE_\" + ty) for ty in types]\n",
    "codes_expr = [F.when(F.col(\"CODE\") == code, 1).otherwise(0).alias(\"e_CODE_\" + code) for code in codes]\n",
    "df = df.select(\"ID\", \"TYPE\", \"CODE\", *types_expr+codes_expr)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, I turned to **Pandas** for help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BusinessTravel_Non-Travel</th>\n",
       "      <th>BusinessTravel_Travel_Frequently</th>\n",
       "      <th>BusinessTravel_Travel_Rarely</th>\n",
       "      <th>Department_Human Resources</th>\n",
       "      <th>Department_Research &amp; Development</th>\n",
       "      <th>Department_Sales</th>\n",
       "      <th>EducationField_Human Resources</th>\n",
       "      <th>EducationField_Life Sciences</th>\n",
       "      <th>EducationField_Marketing</th>\n",
       "      <th>EducationField_Medical</th>\n",
       "      <th>...</th>\n",
       "      <th>JobRole_Manufacturing Director</th>\n",
       "      <th>JobRole_Research Director</th>\n",
       "      <th>JobRole_Research Scientist</th>\n",
       "      <th>JobRole_Sales Executive</th>\n",
       "      <th>JobRole_Sales Representative</th>\n",
       "      <th>MaritalStatus_Divorced</th>\n",
       "      <th>MaritalStatus_Married</th>\n",
       "      <th>MaritalStatus_Single</th>\n",
       "      <th>OverTime_No</th>\n",
       "      <th>OverTime_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BusinessTravel_Non-Travel  BusinessTravel_Travel_Frequently  \\\n",
       "0                          0                                 0   \n",
       "1                          0                                 1   \n",
       "2                          0                                 0   \n",
       "\n",
       "   BusinessTravel_Travel_Rarely  Department_Human Resources  \\\n",
       "0                             1                           0   \n",
       "1                             0                           0   \n",
       "2                             1                           0   \n",
       "\n",
       "   Department_Research & Development  Department_Sales  \\\n",
       "0                                  0                 1   \n",
       "1                                  1                 0   \n",
       "2                                  1                 0   \n",
       "\n",
       "   EducationField_Human Resources  EducationField_Life Sciences  \\\n",
       "0                               0                             1   \n",
       "1                               0                             1   \n",
       "2                               0                             0   \n",
       "\n",
       "   EducationField_Marketing  EducationField_Medical      ...       \\\n",
       "0                         0                       0      ...        \n",
       "1                         0                       0      ...        \n",
       "2                         0                       0      ...        \n",
       "\n",
       "   JobRole_Manufacturing Director  JobRole_Research Director  \\\n",
       "0                               0                          0   \n",
       "1                               0                          0   \n",
       "2                               0                          0   \n",
       "\n",
       "   JobRole_Research Scientist  JobRole_Sales Executive  \\\n",
       "0                           0                        1   \n",
       "1                           1                        0   \n",
       "2                           0                        0   \n",
       "\n",
       "   JobRole_Sales Representative  MaritalStatus_Divorced  \\\n",
       "0                             0                       0   \n",
       "1                             0                       0   \n",
       "2                             0                       0   \n",
       "\n",
       "   MaritalStatus_Married  MaritalStatus_Single  OverTime_No  OverTime_Yes  \n",
       "0                      0                     1            0             1  \n",
       "1                      1                     0            1             0  \n",
       "2                      0                     1            0             1  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pandas to get_dummies\n",
    "import pandas as pd\n",
    "pd_cat = pd.get_dummies(ibm_hr_cat.select(\"*\").toPandas())\n",
    "pd_cat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------------------------------+----------------------------+--------------------------+---------------------------------+----------------+------------------------------+----------------------------+------------------------+----------------------+--------------------+-------------------------------+-------------+-----------+---------------------------------+-----------------------+-----------------------------+---------------+------------------------------+-------------------------+--------------------------+-----------------------+----------------------------+----------------------+---------------------+--------------------+-----------+------------+\n",
      "|BusinessTravel_Non-Travel|BusinessTravel_Travel_Frequently|BusinessTravel_Travel_Rarely|Department_Human Resources|Department_Research & Development|Department_Sales|EducationField_Human Resources|EducationField_Life Sciences|EducationField_Marketing|EducationField_Medical|EducationField_Other|EducationField_Technical Degree|Gender_Female|Gender_Male|JobRole_Healthcare Representative|JobRole_Human Resources|JobRole_Laboratory Technician|JobRole_Manager|JobRole_Manufacturing Director|JobRole_Research Director|JobRole_Research Scientist|JobRole_Sales Executive|JobRole_Sales Representative|MaritalStatus_Divorced|MaritalStatus_Married|MaritalStatus_Single|OverTime_No|OverTime_Yes|\n",
      "+-------------------------+--------------------------------+----------------------------+--------------------------+---------------------------------+----------------+------------------------------+----------------------------+------------------------+----------------------+--------------------+-------------------------------+-------------+-----------+---------------------------------+-----------------------+-----------------------------+---------------+------------------------------+-------------------------+--------------------------+-----------------------+----------------------------+----------------------+---------------------+--------------------+-----------+------------+\n",
      "|                        0|                               0|                           1|                         0|                                0|               1|                             0|                           1|                       0|                     0|                   0|                              0|            1|          0|                                0|                      0|                            0|              0|                             0|                        0|                         0|                      1|                           0|                     0|                    0|                   1|          0|           1|\n",
      "|                        0|                               1|                           0|                         0|                                1|               0|                             0|                           1|                       0|                     0|                   0|                              0|            0|          1|                                0|                      0|                            0|              0|                             0|                        0|                         1|                      0|                           0|                     0|                    1|                   0|          1|           0|\n",
      "|                        0|                               0|                           1|                         0|                                1|               0|                             0|                           0|                       0|                     0|                   1|                              0|            0|          1|                                0|                      0|                            1|              0|                             0|                        0|                         0|                      0|                           0|                     0|                    0|                   1|          0|           1|\n",
      "+-------------------------+--------------------------------+----------------------------+--------------------------+---------------------------------+----------------+------------------------------+----------------------------+------------------------+----------------------+--------------------+-------------------------------+-------------+-----------+---------------------------------+-----------------------+-----------------------------+---------------+------------------------------+-------------------------+--------------------------+-----------------------+----------------------------+----------------------+---------------------+--------------------+-----------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert back to spark dataframe\n",
    "ibm_hr_cat_dum = spark.createDataFrame(pd_cat)\n",
    "ibm_hr_cat_dum.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat the dataframe\n",
    "This is where we are going to train and test our data on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------------+---------+--------------+-----------------------+----------+--------------+--------+---------------+-------------+-----------+------------------+-----------------+-----------------+------------------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "|Age|DailyRate|DistanceFromHome|Education|EmployeeNumber|EnvironmentSatisfaction|HourlyRate|JobInvolvement|JobLevel|JobSatisfaction|MonthlyIncome|MonthlyRate|NumCompaniesWorked|PercentSalaryHike|PerformanceRating|RelationshipSatisfaction|StockOptionLevel|TotalWorkingYears|TrainingTimesLastYear|WorkLifeBalance|YearsAtCompany|YearsInCurrentRole|YearsSinceLastPromotion|YearsWithCurrManager|\n",
      "+---+---------+----------------+---------+--------------+-----------------------+----------+--------------+--------+---------------+-------------+-----------+------------------+-----------------+-----------------+------------------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "| 41|     1102|               1|        2|             1|                      2|        94|             3|       2|              4|         5993|      19479|                 8|               11|                3|                       1|               0|                8|                    0|              1|             6|                 4|                      0|                   5|\n",
      "| 49|      279|               8|        1|             2|                      3|        61|             2|       2|              2|         5130|      24907|                 1|               23|                4|                       4|               1|               10|                    3|              3|            10|                 7|                      1|                   7|\n",
      "| 37|     1373|               2|        2|             4|                      4|        92|             2|       1|              3|         2090|       2396|                 6|               15|                3|                       2|               0|                7|                    3|              3|             0|                 0|                      0|                   0|\n",
      "+---+---------+----------------+---------+--------------+-----------------------+----------+--------------+--------+---------------+-------------+-----------+------------------+-----------------+-----------------+------------------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get numerical features\n",
    "ibm_hr_int = ibm_hr_target.select(numerical)\n",
    "ibm_hr_int.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- DailyRate: integer (nullable = true)\n",
      " |-- DistanceFromHome: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- EmployeeNumber: integer (nullable = true)\n",
      " |-- EnvironmentSatisfaction: integer (nullable = true)\n",
      " |-- HourlyRate: integer (nullable = true)\n",
      " |-- JobInvolvement: integer (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- MonthlyIncome: integer (nullable = true)\n",
      " |-- MonthlyRate: integer (nullable = true)\n",
      " |-- NumCompaniesWorked: integer (nullable = true)\n",
      " |-- PercentSalaryHike: integer (nullable = true)\n",
      " |-- PerformanceRating: integer (nullable = true)\n",
      " |-- RelationshipSatisfaction: integer (nullable = true)\n",
      " |-- StockOptionLevel: integer (nullable = true)\n",
      " |-- TotalWorkingYears: integer (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: integer (nullable = true)\n",
      " |-- YearsAtCompany: integer (nullable = true)\n",
      " |-- YearsInCurrentRole: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- YearsWithCurrManager: integer (nullable = true)\n",
      " |-- BusinessTravel_Non-Travel: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Frequently: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Rarely: integer (nullable = true)\n",
      " |-- Department_Human Resources: integer (nullable = true)\n",
      " |-- Department_Research & Development: integer (nullable = true)\n",
      " |-- Department_Sales: integer (nullable = true)\n",
      " |-- EducationField_Human Resources: integer (nullable = true)\n",
      " |-- EducationField_Life Sciences: integer (nullable = true)\n",
      " |-- EducationField_Marketing: integer (nullable = true)\n",
      " |-- EducationField_Medical: integer (nullable = true)\n",
      " |-- EducationField_Other: integer (nullable = true)\n",
      " |-- EducationField_Technical Degree: integer (nullable = true)\n",
      " |-- Gender_Female: integer (nullable = true)\n",
      " |-- Gender_Male: integer (nullable = true)\n",
      " |-- JobRole_Healthcare Representative: integer (nullable = true)\n",
      " |-- JobRole_Human Resources: integer (nullable = true)\n",
      " |-- JobRole_Laboratory Technician: integer (nullable = true)\n",
      " |-- JobRole_Manager: integer (nullable = true)\n",
      " |-- JobRole_Manufacturing Director: integer (nullable = true)\n",
      " |-- JobRole_Research Director: integer (nullable = true)\n",
      " |-- JobRole_Research Scientist: integer (nullable = true)\n",
      " |-- JobRole_Sales Executive: integer (nullable = true)\n",
      " |-- JobRole_Sales Representative: integer (nullable = true)\n",
      " |-- MaritalStatus_Divorced: integer (nullable = true)\n",
      " |-- MaritalStatus_Married: integer (nullable = true)\n",
      " |-- MaritalStatus_Single: integer (nullable = true)\n",
      " |-- OverTime_No: integer (nullable = true)\n",
      " |-- OverTime_Yes: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concat the two dataframes together columnwise\n",
    "ibm_hr_final = ibm_hr_int.join(ibm_hr_cat_dum)\n",
    "for c in ibm_hr_final.columns:\n",
    "    ibm_hr_final = ibm_hr_final.withColumn(c, ibm_hr_final[c].cast(IntegerType()))\n",
    "ibm_hr_final.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, plot is also difficult with *pyspark* in *jupyter notebook*.\n",
    "\n",
    "Using *Zeppelin* might have a better result, but she does not works well with *Windows*.\n",
    "\n",
    "So at here, I only do a brief stat on our target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regarding attrition:\n",
      "Yes: 237\n",
      "No: 1233\n"
     ]
    }
   ],
   "source": [
    "print(\"regarding attrition:\")\n",
    "print(\"Yes: \" + str(ibm_hr.filter(ibm_hr['Attrition'] == \"Yes\").count()))\n",
    "print(\"No: \" + str(ibm_hr.filter(ibm_hr['Attrition'] == \"No\").count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trial for user self-defined get dummny function\n",
    "Reference can be found at [here](https://runawayhorse001.github.io/LearningApacheSpark/pyspark.pdf).\n",
    "\n",
    "This should work with *pyspark*; however, the error is currently beyond my level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- DailyRate: integer (nullable = true)\n",
      " |-- DistanceFromHome: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- EmployeeNumber: integer (nullable = true)\n",
      " |-- EnvironmentSatisfaction: integer (nullable = true)\n",
      " |-- HourlyRate: integer (nullable = true)\n",
      " |-- JobInvolvement: integer (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- MonthlyIncome: integer (nullable = true)\n",
      " |-- MonthlyRate: integer (nullable = true)\n",
      " |-- NumCompaniesWorked: integer (nullable = true)\n",
      " |-- PercentSalaryHike: integer (nullable = true)\n",
      " |-- PerformanceRating: integer (nullable = true)\n",
      " |-- RelationshipSatisfaction: integer (nullable = true)\n",
      " |-- StockOptionLevel: integer (nullable = true)\n",
      " |-- TotalWorkingYears: integer (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: integer (nullable = true)\n",
      " |-- YearsAtCompany: integer (nullable = true)\n",
      " |-- YearsInCurrentRole: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- YearsWithCurrManager: integer (nullable = true)\n",
      " |-- BusinessTravel_Non-Travel: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Frequently: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Rarely: integer (nullable = true)\n",
      " |-- Department_Human Resources: integer (nullable = true)\n",
      " |-- Department_Research & Development: integer (nullable = true)\n",
      " |-- Department_Sales: integer (nullable = true)\n",
      " |-- EducationField_Human Resources: integer (nullable = true)\n",
      " |-- EducationField_Life Sciences: integer (nullable = true)\n",
      " |-- EducationField_Marketing: integer (nullable = true)\n",
      " |-- EducationField_Medical: integer (nullable = true)\n",
      " |-- EducationField_Other: integer (nullable = true)\n",
      " |-- EducationField_Technical Degree: integer (nullable = true)\n",
      " |-- Gender_Female: integer (nullable = true)\n",
      " |-- Gender_Male: integer (nullable = true)\n",
      " |-- JobRole_Healthcare Representative: integer (nullable = true)\n",
      " |-- JobRole_Human Resources: integer (nullable = true)\n",
      " |-- JobRole_Laboratory Technician: integer (nullable = true)\n",
      " |-- JobRole_Manager: integer (nullable = true)\n",
      " |-- JobRole_Manufacturing Director: integer (nullable = true)\n",
      " |-- JobRole_Research Director: integer (nullable = true)\n",
      " |-- JobRole_Research Scientist: integer (nullable = true)\n",
      " |-- JobRole_Sales Executive: integer (nullable = true)\n",
      " |-- JobRole_Sales Representative: integer (nullable = true)\n",
      " |-- MaritalStatus_Divorced: integer (nullable = true)\n",
      " |-- MaritalStatus_Married: integer (nullable = true)\n",
      " |-- MaritalStatus_Single: integer (nullable = true)\n",
      " |-- OverTime_No: integer (nullable = true)\n",
      " |-- OverTime_Yes: integer (nullable = true)\n",
      " |-- Attrition: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Concat the two dataframes together columnwise\n",
    "ibm_hr_final2 = ibm_hr_final.join(ibm_hr.select(\"Attrition\"))\n",
    "ibm_hr_final2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "def get_dummy(df, categoricalCols, continuousCols, labelCol):\n",
    "    indexers = [StringIndexer(inputCol = c, outputCol = \"{0}_indexed\".format(c)) for c in categoricalCols]\n",
    "    # default setting: dropLast = True\n",
    "    encoders = [OneHotEncoder(inputCol = indexer.getOutputCol(), \\\n",
    "                outputCol = \"{0}_encoded\".format(indexer.getOutputCol())) \\\n",
    "                for indexer in indexers]\n",
    "    assembler = VectorAssembler(inputCols = [encoder.getOutputCol() for encoder in encoders] \\\n",
    "                + continuousCols, outputCol = \"features\")\n",
    "    pipeline = Pipeline(stages=indexers + encoders + [assembler])\n",
    "    model=pipeline.fit(df)\n",
    "    data = model.transform(df)\n",
    "    data = data.withColumn('label', col(labelCol))\n",
    "    return data.select('features', 'label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BusinessTravel',\n",
       " 'Department',\n",
       " 'EducationField',\n",
       " 'Gender',\n",
       " 'JobRole',\n",
       " 'MaritalStatus',\n",
       " 'OverTime']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_no_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'DailyRate',\n",
       " 'DistanceFromHome',\n",
       " 'Education',\n",
       " 'EmployeeNumber',\n",
       " 'EnvironmentSatisfaction',\n",
       " 'HourlyRate',\n",
       " 'JobInvolvement',\n",
       " 'JobLevel',\n",
       " 'JobSatisfaction',\n",
       " 'MonthlyIncome',\n",
       " 'MonthlyRate',\n",
       " 'NumCompaniesWorked',\n",
       " 'PercentSalaryHike',\n",
       " 'PerformanceRating',\n",
       " 'RelationshipSatisfaction',\n",
       " 'StockOptionLevel',\n",
       " 'TotalWorkingYears',\n",
       " 'TrainingTimesLastYear',\n",
       " 'WorkLifeBalance',\n",
       " 'YearsAtCompany',\n",
       " 'YearsInCurrentRole',\n",
       " 'YearsSinceLastPromotion',\n",
       " 'YearsWithCurrManager']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Attrition'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label = \"Attrition\"\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "'Field \"BusinessTravel\" does not exist.\\nAvailable fields: Age, DailyRate, DistanceFromHome, Education, EmployeeNumber, EnvironmentSatisfaction, HourlyRate, JobInvolvement, JobLevel, JobSatisfaction, MonthlyIncome, MonthlyRate, NumCompaniesWorked, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager, BusinessTravel_Non-Travel, BusinessTravel_Travel_Frequently, BusinessTravel_Travel_Rarely, Department_Human Resources, Department_Research & Development, Department_Sales, EducationField_Human Resources, EducationField_Life Sciences, EducationField_Marketing, EducationField_Medical, EducationField_Other, EducationField_Technical Degree, Gender_Female, Gender_Male, JobRole_Healthcare Representative, JobRole_Human Resources, JobRole_Laboratory Technician, JobRole_Manager, JobRole_Manufacturing Director, JobRole_Research Director, JobRole_Research Scientist, JobRole_Sales Executive, JobRole_Sales Representative, MaritalStatus_Divorced, MaritalStatus_Married, MaritalStatus_Single, OverTime_No, OverTime_Yes, Attrition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o745.fit.\n: java.lang.IllegalArgumentException: Field \"BusinessTravel\" does not exist.\nAvailable fields: Age, DailyRate, DistanceFromHome, Education, EmployeeNumber, EnvironmentSatisfaction, HourlyRate, JobInvolvement, JobLevel, JobSatisfaction, MonthlyIncome, MonthlyRate, NumCompaniesWorked, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager, BusinessTravel_Non-Travel, BusinessTravel_Travel_Frequently, BusinessTravel_Travel_Rarely, Department_Human Resources, Department_Research & Development, Department_Sales, EducationField_Human Resources, EducationField_Life Sciences, EducationField_Marketing, EducationField_Medical, EducationField_Other, EducationField_Technical Degree, Gender_Female, Gender_Male, JobRole_Healthcare Representative, JobRole_Human Resources, JobRole_Laboratory Technician, JobRole_Manager, JobRole_Manufacturing Director, JobRole_Research Director, JobRole_Research Scientist, JobRole_Sales Executive, JobRole_Sales Representative, MaritalStatus_Divorced, MaritalStatus_Married, MaritalStatus_Single, OverTime_No, OverTime_Yes, Attrition\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:267)\r\n\tat org.apache.spark.sql.types.StructType$$anonfun$apply$1.apply(StructType.scala:267)\r\n\tat scala.collection.MapLike$class.getOrElse(MapLike.scala:128)\r\n\tat scala.collection.AbstractMap.getOrElse(Map.scala:59)\r\n\tat org.apache.spark.sql.types.StructType.apply(StructType.scala:266)\r\n\tat org.apache.spark.ml.feature.StringIndexerBase$class.validateAndTransformSchema(StringIndexer.scala:85)\r\n\tat org.apache.spark.ml.feature.StringIndexer.validateAndTransformSchema(StringIndexer.scala:109)\r\n\tat org.apache.spark.ml.feature.StringIndexer.transformSchema(StringIndexer.scala:152)\r\n\tat org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:135)\r\n\tat org.apache.spark.ml.feature.StringIndexer.fit(StringIndexer.scala:109)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-20c9d4823668>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_dummy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mibm_hr_final2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_no_target\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mnumerical\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-95f67489b480>\u001b[0m in \u001b[0;36mget_dummy\u001b[1;34m(df, categoricalCols, continuousCols, labelCol)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0massembler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVectorAssembler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputCols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mencoder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetOutputCol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mencoder\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mencoders\u001b[0m\u001b[1;33m]\u001b[0m                 \u001b[1;33m+\u001b[0m \u001b[0mcontinuousCols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"features\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindexers\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mencoders\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0massembler\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \"\"\"\n\u001b[0;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mQueryExecutionException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'java.lang.IllegalArgumentException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 79\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mIllegalArgumentException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     80\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: 'Field \"BusinessTravel\" does not exist.\\nAvailable fields: Age, DailyRate, DistanceFromHome, Education, EmployeeNumber, EnvironmentSatisfaction, HourlyRate, JobInvolvement, JobLevel, JobSatisfaction, MonthlyIncome, MonthlyRate, NumCompaniesWorked, PercentSalaryHike, PerformanceRating, RelationshipSatisfaction, StockOptionLevel, TotalWorkingYears, TrainingTimesLastYear, WorkLifeBalance, YearsAtCompany, YearsInCurrentRole, YearsSinceLastPromotion, YearsWithCurrManager, BusinessTravel_Non-Travel, BusinessTravel_Travel_Frequently, BusinessTravel_Travel_Rarely, Department_Human Resources, Department_Research & Development, Department_Sales, EducationField_Human Resources, EducationField_Life Sciences, EducationField_Marketing, EducationField_Medical, EducationField_Other, EducationField_Technical Degree, Gender_Female, Gender_Male, JobRole_Healthcare Representative, JobRole_Human Resources, JobRole_Laboratory Technician, JobRole_Manager, JobRole_Manufacturing Director, JobRole_Research Director, JobRole_Research Scientist, JobRole_Sales Executive, JobRole_Sales Representative, MaritalStatus_Divorced, MaritalStatus_Married, MaritalStatus_Single, OverTime_No, OverTime_Yes, Attrition'"
     ]
    }
   ],
   "source": [
    "data = get_dummy(ibm_hr_final2, categorical_no_target ,numerical, label)\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Implementing Machine Learning Models\n",
    "As we have finished our data exploration process and simple feature engineering with regards to categorical values, we can hereby try to build our own models.\n",
    "\n",
    "### Brief preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- DailyRate: integer (nullable = true)\n",
      " |-- DistanceFromHome: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- EmployeeNumber: integer (nullable = true)\n",
      " |-- EnvironmentSatisfaction: integer (nullable = true)\n",
      " |-- HourlyRate: integer (nullable = true)\n",
      " |-- JobInvolvement: integer (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- MonthlyIncome: integer (nullable = true)\n",
      " |-- MonthlyRate: integer (nullable = true)\n",
      " |-- NumCompaniesWorked: integer (nullable = true)\n",
      " |-- PercentSalaryHike: integer (nullable = true)\n",
      " |-- PerformanceRating: integer (nullable = true)\n",
      " |-- RelationshipSatisfaction: integer (nullable = true)\n",
      " |-- StockOptionLevel: integer (nullable = true)\n",
      " |-- TotalWorkingYears: integer (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: integer (nullable = true)\n",
      " |-- YearsAtCompany: integer (nullable = true)\n",
      " |-- YearsInCurrentRole: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- YearsWithCurrManager: integer (nullable = true)\n",
      " |-- BusinessTravel_Non-Travel: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Frequently: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Rarely: integer (nullable = true)\n",
      " |-- Department_Human Resources: integer (nullable = true)\n",
      " |-- Department_Research & Development: integer (nullable = true)\n",
      " |-- Department_Sales: integer (nullable = true)\n",
      " |-- EducationField_Human Resources: integer (nullable = true)\n",
      " |-- EducationField_Life Sciences: integer (nullable = true)\n",
      " |-- EducationField_Marketing: integer (nullable = true)\n",
      " |-- EducationField_Medical: integer (nullable = true)\n",
      " |-- EducationField_Other: integer (nullable = true)\n",
      " |-- EducationField_Technical Degree: integer (nullable = true)\n",
      " |-- Gender_Female: integer (nullable = true)\n",
      " |-- Gender_Male: integer (nullable = true)\n",
      " |-- JobRole_Healthcare Representative: integer (nullable = true)\n",
      " |-- JobRole_Human Resources: integer (nullable = true)\n",
      " |-- JobRole_Laboratory Technician: integer (nullable = true)\n",
      " |-- JobRole_Manager: integer (nullable = true)\n",
      " |-- JobRole_Manufacturing Director: integer (nullable = true)\n",
      " |-- JobRole_Research Director: integer (nullable = true)\n",
      " |-- JobRole_Research Scientist: integer (nullable = true)\n",
      " |-- JobRole_Sales Executive: integer (nullable = true)\n",
      " |-- JobRole_Sales Representative: integer (nullable = true)\n",
      " |-- MaritalStatus_Divorced: integer (nullable = true)\n",
      " |-- MaritalStatus_Married: integer (nullable = true)\n",
      " |-- MaritalStatus_Single: integer (nullable = true)\n",
      " |-- OverTime_No: integer (nullable = true)\n",
      " |-- OverTime_Yes: integer (nullable = true)\n",
      " |-- Attrition_numerical: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get the final train dataset\n",
    "ibm_train = ibm_hr_final.join(ibm_hr_target.select(\"Attrition_numerical\"))\n",
    "ibm_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age',\n",
       " 'DailyRate',\n",
       " 'DistanceFromHome',\n",
       " 'Education',\n",
       " 'EmployeeNumber',\n",
       " 'EnvironmentSatisfaction',\n",
       " 'HourlyRate',\n",
       " 'JobInvolvement',\n",
       " 'JobLevel',\n",
       " 'JobSatisfaction',\n",
       " 'MonthlyIncome',\n",
       " 'MonthlyRate',\n",
       " 'NumCompaniesWorked',\n",
       " 'PercentSalaryHike',\n",
       " 'PerformanceRating',\n",
       " 'RelationshipSatisfaction',\n",
       " 'StockOptionLevel',\n",
       " 'TotalWorkingYears',\n",
       " 'TrainingTimesLastYear',\n",
       " 'WorkLifeBalance',\n",
       " 'YearsAtCompany',\n",
       " 'YearsInCurrentRole',\n",
       " 'YearsSinceLastPromotion',\n",
       " 'YearsWithCurrManager',\n",
       " 'BusinessTravel_Non-Travel',\n",
       " 'BusinessTravel_Travel_Frequently',\n",
       " 'BusinessTravel_Travel_Rarely',\n",
       " 'Department_Human Resources',\n",
       " 'Department_Research & Development',\n",
       " 'Department_Sales',\n",
       " 'EducationField_Human Resources',\n",
       " 'EducationField_Life Sciences',\n",
       " 'EducationField_Marketing',\n",
       " 'EducationField_Medical',\n",
       " 'EducationField_Other',\n",
       " 'EducationField_Technical Degree',\n",
       " 'Gender_Female',\n",
       " 'Gender_Male',\n",
       " 'JobRole_Healthcare Representative',\n",
       " 'JobRole_Human Resources',\n",
       " 'JobRole_Laboratory Technician',\n",
       " 'JobRole_Manager',\n",
       " 'JobRole_Manufacturing Director',\n",
       " 'JobRole_Research Director',\n",
       " 'JobRole_Research Scientist',\n",
       " 'JobRole_Sales Executive',\n",
       " 'JobRole_Sales Representative',\n",
       " 'MaritalStatus_Divorced',\n",
       " 'MaritalStatus_Married',\n",
       " 'MaritalStatus_Single',\n",
       " 'OverTime_No',\n",
       " 'OverTime_Yes']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the feature column\n",
    "feature_col = ibm_hr_final.columns\n",
    "feature_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53, 52)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ibm_train.columns), len(feature_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split and assemble train and test dataset\n",
    "reference can be found at [here](https://d3c33hcgiwev3.cloudfront.net/_9826a141dcc6330114b430d550e6530b_ClassificationinSpark.pdf?Expires=1533168000&Signature=M355ag~GIntDzDHOYDlyCrbryBW9S~TtgfW3B~qeNnfq5Ucp9419XN-wX7uV5wKgooI9S47BGJdxeCzGZNBxaNKwfd6PqXLlHPxbj2~OR0ulPqMKYOU59hlk9IaSsZZ1JcsQsJDS1hb0ufIJIq5WVrxF6-xfXGPX1q87IM60CCg_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# essential imports\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# assemble features\n",
    "assembler = VectorAssembler(inputCols = feature_col, outputCol = \"features\")\n",
    "assembled = assembler.transform(ibm_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "(training_data, testing_data) = assembled.randomSplit([0.8, 0.2], seed = 13234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(labelCol = \"Attrition_numerical\", featuresCol = \"features\", \\\n",
    "                           maxDepth = 7, minInstancesPerNode = 20, impurity = \"gini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Detected implicit cartesian product for INNER join between logical plans\\nProject [cast(Age#10 as int) AS Age#1252, cast(DailyRate#13 as int) AS DailyRate#1305, cast(DistanceFromHome#15 as int) AS DistanceFromHome#1358, cast(Education#16 as int) AS Education#1411, cast(EmployeeNumber#19 as int) AS EmployeeNumber#1464, cast(EnvironmentSatisfaction#20 as int) AS EnvironmentSatisfaction#1517, cast(HourlyRate#22 as int) AS HourlyRate#1570, cast(JobInvolvement#23 as int) AS JobInvolvement#1623, cast(JobLevel#24 as int) AS JobLevel#1676, cast(JobSatisfaction#26 as int) AS JobSatisfaction#1729, cast(MonthlyIncome#28 as int) AS MonthlyIncome#1782, cast(MonthlyRate#29 as int) AS MonthlyRate#1835, cast(NumCompaniesWorked#30 as int) AS NumCompaniesWorked#1888, cast(PercentSalaryHike#33 as int) AS PercentSalaryHike#1941, cast(PerformanceRating#34 as int) AS PerformanceRating#1994, cast(RelationshipSatisfaction#35 as int) AS RelationshipSatisfaction#2047, cast(StockOptionLevel#37 as int) AS StockOptionLevel#2100, cast(TotalWorkingYears#38 as int) AS TotalWorkingYears#2153, cast(TrainingTimesLastYear#39 as int) AS TrainingTimesLastYear#2206, cast(WorkLifeBalance#40 as int) AS WorkLifeBalance#2259, cast(YearsAtCompany#41 as int) AS YearsAtCompany#2312, cast(YearsInCurrentRole#42 as int) AS YearsInCurrentRole#2365, cast(YearsSinceLastPromotion#43 as int) AS YearsSinceLastPromotion#2418, cast(YearsWithCurrManager#44 as int) AS YearsWithCurrManager#2471, ... 28 more fields]\\n+- Join Inner\\n   :- Project [Age#10, DailyRate#13, DistanceFromHome#15, Education#16, EmployeeNumber#19, EnvironmentSatisfaction#20, HourlyRate#22, JobInvolvement#23, JobLevel#24, JobSatisfaction#26, MonthlyIncome#28, MonthlyRate#29, NumCompaniesWorked#30, PercentSalaryHike#33, PerformanceRating#34, RelationshipSatisfaction#35, StockOptionLevel#37, TotalWorkingYears#38, TrainingTimesLastYear#39, WorkLifeBalance#40, YearsAtCompany#41, YearsInCurrentRole#42, YearsSinceLastPromotion#43, YearsWithCurrManager#44]\\n   :  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n   +- LogicalRDD [BusinessTravel_Non-Travel#936L, BusinessTravel_Travel_Frequently#937L, BusinessTravel_Travel_Rarely#938L, Department_Human Resources#939L, Department_Research & Development#940L, Department_Sales#941L, EducationField_Human Resources#942L, EducationField_Life Sciences#943L, EducationField_Marketing#944L, EducationField_Medical#945L, EducationField_Other#946L, EducationField_Technical Degree#947L, Gender_Female#948L, Gender_Male#949L, JobRole_Healthcare Representative#950L, JobRole_Human Resources#951L, JobRole_Laboratory Technician#952L, JobRole_Manager#953L, JobRole_Manufacturing Director#954L, JobRole_Research Director#955L, JobRole_Research Scientist#956L, JobRole_Sales Executive#957L, JobRole_Sales Representative#958L, MaritalStatus_Divorced#959L, ... 4 more fields], false\\nand\\nProject [bool_to_int(Attrition#11) AS Attrition_numerical#546]\\n+- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o1247.fit.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nProject [cast(Age#10 as int) AS Age#1252, cast(DailyRate#13 as int) AS DailyRate#1305, cast(DistanceFromHome#15 as int) AS DistanceFromHome#1358, cast(Education#16 as int) AS Education#1411, cast(EmployeeNumber#19 as int) AS EmployeeNumber#1464, cast(EnvironmentSatisfaction#20 as int) AS EnvironmentSatisfaction#1517, cast(HourlyRate#22 as int) AS HourlyRate#1570, cast(JobInvolvement#23 as int) AS JobInvolvement#1623, cast(JobLevel#24 as int) AS JobLevel#1676, cast(JobSatisfaction#26 as int) AS JobSatisfaction#1729, cast(MonthlyIncome#28 as int) AS MonthlyIncome#1782, cast(MonthlyRate#29 as int) AS MonthlyRate#1835, cast(NumCompaniesWorked#30 as int) AS NumCompaniesWorked#1888, cast(PercentSalaryHike#33 as int) AS PercentSalaryHike#1941, cast(PerformanceRating#34 as int) AS PerformanceRating#1994, cast(RelationshipSatisfaction#35 as int) AS RelationshipSatisfaction#2047, cast(StockOptionLevel#37 as int) AS StockOptionLevel#2100, cast(TotalWorkingYears#38 as int) AS TotalWorkingYears#2153, cast(TrainingTimesLastYear#39 as int) AS TrainingTimesLastYear#2206, cast(WorkLifeBalance#40 as int) AS WorkLifeBalance#2259, cast(YearsAtCompany#41 as int) AS YearsAtCompany#2312, cast(YearsInCurrentRole#42 as int) AS YearsInCurrentRole#2365, cast(YearsSinceLastPromotion#43 as int) AS YearsSinceLastPromotion#2418, cast(YearsWithCurrManager#44 as int) AS YearsWithCurrManager#2471, ... 28 more fields]\n+- Join Inner\n   :- Project [Age#10, DailyRate#13, DistanceFromHome#15, Education#16, EmployeeNumber#19, EnvironmentSatisfaction#20, HourlyRate#22, JobInvolvement#23, JobLevel#24, JobSatisfaction#26, MonthlyIncome#28, MonthlyRate#29, NumCompaniesWorked#30, PercentSalaryHike#33, PerformanceRating#34, RelationshipSatisfaction#35, StockOptionLevel#37, TotalWorkingYears#38, TrainingTimesLastYear#39, WorkLifeBalance#40, YearsAtCompany#41, YearsInCurrentRole#42, YearsSinceLastPromotion#43, YearsWithCurrManager#44]\n   :  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\n   +- LogicalRDD [BusinessTravel_Non-Travel#936L, BusinessTravel_Travel_Frequently#937L, BusinessTravel_Travel_Rarely#938L, Department_Human Resources#939L, Department_Research & Development#940L, Department_Sales#941L, EducationField_Human Resources#942L, EducationField_Life Sciences#943L, EducationField_Marketing#944L, EducationField_Medical#945L, EducationField_Other#946L, EducationField_Technical Degree#947L, Gender_Female#948L, Gender_Male#949L, JobRole_Healthcare Representative#950L, JobRole_Human Resources#951L, JobRole_Laboratory Technician#952L, JobRole_Manager#953L, JobRole_Manufacturing Director#954L, JobRole_Research Director#955L, JobRole_Research Scientist#956L, JobRole_Sales Executive#957L, JobRole_Sales Representative#958L, MaritalStatus_Divorced#959L, ... 4 more fields], false\nand\nProject [bool_to_int(Attrition#11) AS Attrition_numerical#546]\n+- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1124)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1121)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1121)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1103)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\r\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3249)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2484)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2698)\r\n\tat org.apache.spark.ml.classification.Classifier.getNumClasses(Classifier.scala:111)\r\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:102)\r\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:45)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-4fb44e63c39e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# must be an Estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m                     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    130\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 132\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mjava_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\ml\\wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    283\u001b[0m         \"\"\"\n\u001b[0;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 285\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for INNER join between logical plans\\nProject [cast(Age#10 as int) AS Age#1252, cast(DailyRate#13 as int) AS DailyRate#1305, cast(DistanceFromHome#15 as int) AS DistanceFromHome#1358, cast(Education#16 as int) AS Education#1411, cast(EmployeeNumber#19 as int) AS EmployeeNumber#1464, cast(EnvironmentSatisfaction#20 as int) AS EnvironmentSatisfaction#1517, cast(HourlyRate#22 as int) AS HourlyRate#1570, cast(JobInvolvement#23 as int) AS JobInvolvement#1623, cast(JobLevel#24 as int) AS JobLevel#1676, cast(JobSatisfaction#26 as int) AS JobSatisfaction#1729, cast(MonthlyIncome#28 as int) AS MonthlyIncome#1782, cast(MonthlyRate#29 as int) AS MonthlyRate#1835, cast(NumCompaniesWorked#30 as int) AS NumCompaniesWorked#1888, cast(PercentSalaryHike#33 as int) AS PercentSalaryHike#1941, cast(PerformanceRating#34 as int) AS PerformanceRating#1994, cast(RelationshipSatisfaction#35 as int) AS RelationshipSatisfaction#2047, cast(StockOptionLevel#37 as int) AS StockOptionLevel#2100, cast(TotalWorkingYears#38 as int) AS TotalWorkingYears#2153, cast(TrainingTimesLastYear#39 as int) AS TrainingTimesLastYear#2206, cast(WorkLifeBalance#40 as int) AS WorkLifeBalance#2259, cast(YearsAtCompany#41 as int) AS YearsAtCompany#2312, cast(YearsInCurrentRole#42 as int) AS YearsInCurrentRole#2365, cast(YearsSinceLastPromotion#43 as int) AS YearsSinceLastPromotion#2418, cast(YearsWithCurrManager#44 as int) AS YearsWithCurrManager#2471, ... 28 more fields]\\n+- Join Inner\\n   :- Project [Age#10, DailyRate#13, DistanceFromHome#15, Education#16, EmployeeNumber#19, EnvironmentSatisfaction#20, HourlyRate#22, JobInvolvement#23, JobLevel#24, JobSatisfaction#26, MonthlyIncome#28, MonthlyRate#29, NumCompaniesWorked#30, PercentSalaryHike#33, PerformanceRating#34, RelationshipSatisfaction#35, StockOptionLevel#37, TotalWorkingYears#38, TrainingTimesLastYear#39, WorkLifeBalance#40, YearsAtCompany#41, YearsInCurrentRole#42, YearsSinceLastPromotion#43, YearsWithCurrManager#44]\\n   :  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n   +- LogicalRDD [BusinessTravel_Non-Travel#936L, BusinessTravel_Travel_Frequently#937L, BusinessTravel_Travel_Rarely#938L, Department_Human Resources#939L, Department_Research & Development#940L, Department_Sales#941L, EducationField_Human Resources#942L, EducationField_Life Sciences#943L, EducationField_Marketing#944L, EducationField_Medical#945L, EducationField_Other#946L, EducationField_Technical Degree#947L, Gender_Female#948L, Gender_Male#949L, JobRole_Healthcare Representative#950L, JobRole_Human Resources#951L, JobRole_Laboratory Technician#952L, JobRole_Manager#953L, JobRole_Manufacturing Director#954L, JobRole_Research Director#955L, JobRole_Research Scientist#956L, JobRole_Sales Executive#957L, JobRole_Sales Representative#958L, MaritalStatus_Divorced#959L, ... 4 more fields], false\\nand\\nProject [bool_to_int(Attrition#11) AS Attrition_numerical#546]\\n+- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages = [dt])\n",
    "model = pipeline.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
