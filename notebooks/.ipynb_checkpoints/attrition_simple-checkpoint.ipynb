{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# important: please modify the path as your local spark location \n",
    "spark_path = \"D:/spark-2.3.1-bin-hadoop2.7\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spark is well set at 2018-08-01 10:03:07.645657\n"
     ]
    }
   ],
   "source": [
    "# configure spark, a message regrads time will print out if success\n",
    "import findspark\n",
    "from datetime import datetime\n",
    "findspark.init(\"D:/spark-2.3.1-bin-hadoop2.7\")\n",
    "from pyspark import SparkConf, SparkContext\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"ibm_hr_simple\")\n",
    "sc = SparkContext(conf = conf)\n",
    "print(\"spark is well set at \" + str(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Chengzhong:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ibm_hr_simple</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x22c2240d518>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# open spark session\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession(sc)\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+-----------------+---------+--------------------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+--------------------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "|Age|Attrition|   BusinessTravel|DailyRate|          Department|DistanceFromHome|Education|EducationField|EmployeeCount|EmployeeNumber|EnvironmentSatisfaction|Gender|HourlyRate|JobInvolvement|JobLevel|             JobRole|JobSatisfaction|MaritalStatus|MonthlyIncome|MonthlyRate|NumCompaniesWorked|Over18|OverTime|PercentSalaryHike|PerformanceRating|RelationshipSatisfaction|StandardHours|StockOptionLevel|TotalWorkingYears|TrainingTimesLastYear|WorkLifeBalance|YearsAtCompany|YearsInCurrentRole|YearsSinceLastPromotion|YearsWithCurrManager|\n",
      "+---+---------+-----------------+---------+--------------------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+--------------------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "| 41|      Yes|    Travel_Rarely|     1102|               Sales|               1|        2| Life Sciences|            1|             1|                      2|Female|        94|             3|       2|     Sales Executive|              4|       Single|         5993|      19479|                 8|     Y|     Yes|               11|                3|                       1|           80|               0|                8|                    0|              1|             6|                 4|                      0|                   5|\n",
      "| 49|       No|Travel_Frequently|      279|Research & Develo...|               8|        1| Life Sciences|            1|             2|                      3|  Male|        61|             2|       2|  Research Scientist|              2|      Married|         5130|      24907|                 1|     Y|      No|               23|                4|                       4|           80|               1|               10|                    3|              3|            10|                 7|                      1|                   7|\n",
      "| 37|      Yes|    Travel_Rarely|     1373|Research & Develo...|               2|        2|         Other|            1|             4|                      4|  Male|        92|             2|       1|Laboratory Techni...|              3|       Single|         2090|       2396|                 6|     Y|     Yes|               15|                3|                       2|           80|               0|                7|                    3|              3|             0|                 0|                      0|                   0|\n",
      "+---+---------+-----------------+---------+--------------------+----------------+---------+--------------+-------------+--------------+-----------------------+------+----------+--------------+--------+--------------------+---------------+-------------+-------------+-----------+------------------+------+--------+-----------------+-----------------+------------------------+-------------+----------------+-----------------+---------------------+---------------+--------------+------------------+-----------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "ibm_hr = spark.read.csv(\"../data/WA_Fn-UseC_-HR-Employee-Attrition.csv\", header=True, mode=\"DROPMALFORMED\")\n",
    "ibm_hr.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+\n",
      "|Attrition|Attrition_numerical|\n",
      "+---------+-------------------+\n",
      "|      Yes|                  1|\n",
      "|       No|                  0|\n",
      "|      Yes|                  1|\n",
      "+---------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import *\n",
    "# define function to transform boolean\n",
    "def bool_to_int(b):\n",
    "    if b == \"Yes\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "# register user defined function with spark SQL\n",
    "udf_bool_to_int = udf(bool_to_int, IntegerType())\n",
    "# add column\n",
    "ibm_hr_target = ibm_hr.withColumn(\"Attrition_numerical\", udf_bool_to_int(\"Attrition\"))\n",
    "# check the result\n",
    "ibm_hr_target.select(\"Attrition\", \"Attrition_numerical\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1470, 35)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibm_hr_target = ibm_hr_target.drop(\"Attrition\")\n",
    "ibm_hr_target.count(), len(ibm_hr_target.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 24)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical = ['BusinessTravel', 'Department', 'EducationField', 'Gender', 'JobRole', 'MaritalStatus', 'OverTime']\n",
    "numerical = [u'Age', u'DailyRate', u'DistanceFromHome', u'Education', u'EmployeeNumber', u'EnvironmentSatisfaction', \\\n",
    "             u'HourlyRate', u'JobInvolvement', u'JobLevel', u'JobSatisfaction', \\\n",
    "             u'MonthlyIncome', u'MonthlyRate', u'NumCompaniesWorked', \\\n",
    "             u'PercentSalaryHike', u'PerformanceRating', u'RelationshipSatisfaction', \\\n",
    "             u'StockOptionLevel', u'TotalWorkingYears', \\\n",
    "             u'TrainingTimesLastYear', u'WorkLifeBalance', u'YearsAtCompany', \\\n",
    "             u'YearsInCurrentRole', u'YearsSinceLastPromotion', u'YearsWithCurrManager']\n",
    "len(categorical), len(numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BusinessTravel_Non-Travel</th>\n",
       "      <th>BusinessTravel_Travel_Frequently</th>\n",
       "      <th>BusinessTravel_Travel_Rarely</th>\n",
       "      <th>Department_Human Resources</th>\n",
       "      <th>Department_Research &amp; Development</th>\n",
       "      <th>Department_Sales</th>\n",
       "      <th>EducationField_Human Resources</th>\n",
       "      <th>EducationField_Life Sciences</th>\n",
       "      <th>EducationField_Marketing</th>\n",
       "      <th>EducationField_Medical</th>\n",
       "      <th>...</th>\n",
       "      <th>JobRole_Manufacturing Director</th>\n",
       "      <th>JobRole_Research Director</th>\n",
       "      <th>JobRole_Research Scientist</th>\n",
       "      <th>JobRole_Sales Executive</th>\n",
       "      <th>JobRole_Sales Representative</th>\n",
       "      <th>MaritalStatus_Divorced</th>\n",
       "      <th>MaritalStatus_Married</th>\n",
       "      <th>MaritalStatus_Single</th>\n",
       "      <th>OverTime_No</th>\n",
       "      <th>OverTime_Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   BusinessTravel_Non-Travel  BusinessTravel_Travel_Frequently  \\\n",
       "0                          0                                 0   \n",
       "1                          0                                 1   \n",
       "2                          0                                 0   \n",
       "\n",
       "   BusinessTravel_Travel_Rarely  Department_Human Resources  \\\n",
       "0                             1                           0   \n",
       "1                             0                           0   \n",
       "2                             1                           0   \n",
       "\n",
       "   Department_Research & Development  Department_Sales  \\\n",
       "0                                  0                 1   \n",
       "1                                  1                 0   \n",
       "2                                  1                 0   \n",
       "\n",
       "   EducationField_Human Resources  EducationField_Life Sciences  \\\n",
       "0                               0                             1   \n",
       "1                               0                             1   \n",
       "2                               0                             0   \n",
       "\n",
       "   EducationField_Marketing  EducationField_Medical      ...       \\\n",
       "0                         0                       0      ...        \n",
       "1                         0                       0      ...        \n",
       "2                         0                       0      ...        \n",
       "\n",
       "   JobRole_Manufacturing Director  JobRole_Research Director  \\\n",
       "0                               0                          0   \n",
       "1                               0                          0   \n",
       "2                               0                          0   \n",
       "\n",
       "   JobRole_Research Scientist  JobRole_Sales Executive  \\\n",
       "0                           0                        1   \n",
       "1                           1                        0   \n",
       "2                           0                        0   \n",
       "\n",
       "   JobRole_Sales Representative  MaritalStatus_Divorced  \\\n",
       "0                             0                       0   \n",
       "1                             0                       0   \n",
       "2                             0                       0   \n",
       "\n",
       "   MaritalStatus_Married  MaritalStatus_Single  OverTime_No  OverTime_Yes  \n",
       "0                      0                     1            0             1  \n",
       "1                      1                     0            1             0  \n",
       "2                      0                     1            0             1  \n",
       "\n",
       "[3 rows x 28 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pandas to get_dummies\n",
    "import pandas as pd\n",
    "pd_cat = pd.get_dummies(ibm_hr_target.select(categorical).toPandas())\n",
    "pd_cat.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ibm_hr_cat = spark.createDataFrame(pd_cat)\n",
    "for c in ibm_hr_cat.columns:\n",
    "    ibm_hr_cat = ibm_hr_cat.withColumn(c, ibm_hr_cat[c].cast(IntegerType()))\n",
    "ibm_hr_att = ibm_hr_target.select(\"Attrition_numerical\")\n",
    "ibm_hr_target = ibm_hr_target.select(numerical)\n",
    "for c in ibm_hr_target.columns:\n",
    "    ibm_hr_target = ibm_hr_target.withColumn(c, ibm_hr_target[c].cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- DailyRate: integer (nullable = true)\n",
      " |-- DistanceFromHome: integer (nullable = true)\n",
      " |-- Education: integer (nullable = true)\n",
      " |-- EmployeeNumber: integer (nullable = true)\n",
      " |-- EnvironmentSatisfaction: integer (nullable = true)\n",
      " |-- HourlyRate: integer (nullable = true)\n",
      " |-- JobInvolvement: integer (nullable = true)\n",
      " |-- JobLevel: integer (nullable = true)\n",
      " |-- JobSatisfaction: integer (nullable = true)\n",
      " |-- MonthlyIncome: integer (nullable = true)\n",
      " |-- MonthlyRate: integer (nullable = true)\n",
      " |-- NumCompaniesWorked: integer (nullable = true)\n",
      " |-- PercentSalaryHike: integer (nullable = true)\n",
      " |-- PerformanceRating: integer (nullable = true)\n",
      " |-- RelationshipSatisfaction: integer (nullable = true)\n",
      " |-- StockOptionLevel: integer (nullable = true)\n",
      " |-- TotalWorkingYears: integer (nullable = true)\n",
      " |-- TrainingTimesLastYear: integer (nullable = true)\n",
      " |-- WorkLifeBalance: integer (nullable = true)\n",
      " |-- YearsAtCompany: integer (nullable = true)\n",
      " |-- YearsInCurrentRole: integer (nullable = true)\n",
      " |-- YearsSinceLastPromotion: integer (nullable = true)\n",
      " |-- YearsWithCurrManager: integer (nullable = true)\n",
      " |-- BusinessTravel_Non-Travel: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Frequently: integer (nullable = true)\n",
      " |-- BusinessTravel_Travel_Rarely: integer (nullable = true)\n",
      " |-- Department_Human Resources: integer (nullable = true)\n",
      " |-- Department_Research & Development: integer (nullable = true)\n",
      " |-- Department_Sales: integer (nullable = true)\n",
      " |-- EducationField_Human Resources: integer (nullable = true)\n",
      " |-- EducationField_Life Sciences: integer (nullable = true)\n",
      " |-- EducationField_Marketing: integer (nullable = true)\n",
      " |-- EducationField_Medical: integer (nullable = true)\n",
      " |-- EducationField_Other: integer (nullable = true)\n",
      " |-- EducationField_Technical Degree: integer (nullable = true)\n",
      " |-- Gender_Female: integer (nullable = true)\n",
      " |-- Gender_Male: integer (nullable = true)\n",
      " |-- JobRole_Healthcare Representative: integer (nullable = true)\n",
      " |-- JobRole_Human Resources: integer (nullable = true)\n",
      " |-- JobRole_Laboratory Technician: integer (nullable = true)\n",
      " |-- JobRole_Manager: integer (nullable = true)\n",
      " |-- JobRole_Manufacturing Director: integer (nullable = true)\n",
      " |-- JobRole_Research Director: integer (nullable = true)\n",
      " |-- JobRole_Research Scientist: integer (nullable = true)\n",
      " |-- JobRole_Sales Executive: integer (nullable = true)\n",
      " |-- JobRole_Sales Representative: integer (nullable = true)\n",
      " |-- MaritalStatus_Divorced: integer (nullable = true)\n",
      " |-- MaritalStatus_Married: integer (nullable = true)\n",
      " |-- MaritalStatus_Single: integer (nullable = true)\n",
      " |-- OverTime_No: integer (nullable = true)\n",
      " |-- OverTime_Yes: integer (nullable = true)\n",
      " |-- Attrition_numerical: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ibm_hr_target = ibm_hr_target.join(ibm_hr_cat)\n",
    "ibm_hr_target = ibm_hr_target.join(ibm_hr_att)\n",
    "ibm_hr_target.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52, 53)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_col = ibm_hr_target.columns\n",
    "feature_col.remove(\"Attrition_numerical\")\n",
    "len(feature_col), len(ibm_hr_target.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The problem here seems to be with the join\n",
    "to have a clearer view, I'll switch to another notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1470"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibm_hr_cat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1470"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibm_hr_att.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Detected implicit cartesian product for INNER join between logical plans\\nProject\\n+- LogicalRDD [BusinessTravel_Non-Travel#360L, BusinessTravel_Travel_Frequently#361L, BusinessTravel_Travel_Rarely#362L, Department_Human Resources#363L, Department_Research & Development#364L, Department_Sales#365L, EducationField_Human Resources#366L, EducationField_Life Sciences#367L, EducationField_Marketing#368L, EducationField_Medical#369L, EducationField_Other#370L, EducationField_Technical Degree#371L, Gender_Female#372L, Gender_Male#373L, JobRole_Healthcare Representative#374L, JobRole_Human Resources#375L, JobRole_Laboratory Technician#376L, JobRole_Manager#377L, JobRole_Manufacturing Director#378L, JobRole_Research Director#379L, JobRole_Research Scientist#380L, JobRole_Sales Executive#381L, JobRole_Sales Representative#382L, MaritalStatus_Divorced#383L, ... 4 more fields], false\\nand\\nProject\\n+- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o591.count.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nProject\n+- LogicalRDD [BusinessTravel_Non-Travel#360L, BusinessTravel_Travel_Frequently#361L, BusinessTravel_Travel_Rarely#362L, Department_Human Resources#363L, Department_Research & Development#364L, Department_Sales#365L, EducationField_Human Resources#366L, EducationField_Life Sciences#367L, EducationField_Marketing#368L, EducationField_Medical#369L, EducationField_Other#370L, EducationField_Technical Degree#371L, Gender_Female#372L, Gender_Male#373L, JobRole_Healthcare Representative#374L, JobRole_Human Resources#375L, JobRole_Laboratory Technician#376L, JobRole_Manager#377L, JobRole_Manufacturing Director#378L, JobRole_Research Director#379L, JobRole_Research Scientist#380L, JobRole_Sales Executive#381L, JobRole_Sales Representative#382L, MaritalStatus_Divorced#383L, ... 4 more fields], false\nand\nProject\n+- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1124)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1121)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1121)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1103)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\r\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3249)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-fcb7d9cbabff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mibm_hr_cat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mibm_hr_att\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m         \"\"\"\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for INNER join between logical plans\\nProject\\n+- LogicalRDD [BusinessTravel_Non-Travel#360L, BusinessTravel_Travel_Frequently#361L, BusinessTravel_Travel_Rarely#362L, Department_Human Resources#363L, Department_Research & Development#364L, Department_Sales#365L, EducationField_Human Resources#366L, EducationField_Life Sciences#367L, EducationField_Marketing#368L, EducationField_Medical#369L, EducationField_Other#370L, EducationField_Technical Degree#371L, Gender_Female#372L, Gender_Male#373L, JobRole_Healthcare Representative#374L, JobRole_Human Resources#375L, JobRole_Laboratory Technician#376L, JobRole_Manager#377L, JobRole_Manufacturing Director#378L, JobRole_Research Director#379L, JobRole_Research Scientist#380L, JobRole_Sales Executive#381L, JobRole_Sales Representative#382L, MaritalStatus_Divorced#383L, ... 4 more fields], false\\nand\\nProject\\n+- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
     ]
    }
   ],
   "source": [
    "ibm_hr_cat.join(ibm_hr_att).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner\\n:- Join Inner\\n:  :- Join Inner\\n:  :  :- Project [cast(Age#10 as int) AS Age#691, cast(DailyRate#13 as int) AS DailyRate#716, cast(DistanceFromHome#15 as int) AS DistanceFromHome#741, cast(Education#16 as int) AS Education#766, cast(EmployeeNumber#19 as int) AS EmployeeNumber#791, cast(EnvironmentSatisfaction#20 as int) AS EnvironmentSatisfaction#816, cast(HourlyRate#22 as int) AS HourlyRate#841, cast(JobInvolvement#23 as int) AS JobInvolvement#866, cast(JobLevel#24 as int) AS JobLevel#891, cast(JobSatisfaction#26 as int) AS JobSatisfaction#916, cast(MonthlyIncome#28 as int) AS MonthlyIncome#941, cast(MonthlyRate#29 as int) AS MonthlyRate#966, cast(NumCompaniesWorked#30 as int) AS NumCompaniesWorked#991, cast(PercentSalaryHike#33 as int) AS PercentSalaryHike#1016, cast(PerformanceRating#34 as int) AS PerformanceRating#1041, cast(RelationshipSatisfaction#35 as int) AS RelationshipSatisfaction#1066, cast(StockOptionLevel#37 as int) AS StockOptionLevel#1091, cast(TotalWorkingYears#38 as int) AS TotalWorkingYears#1116, cast(TrainingTimesLastYear#39 as int) AS TrainingTimesLastYear#1141, cast(WorkLifeBalance#40 as int) AS WorkLifeBalance#1166, cast(YearsAtCompany#41 as int) AS YearsAtCompany#1191, cast(YearsInCurrentRole#42 as int) AS YearsInCurrentRole#1216, cast(YearsSinceLastPromotion#43 as int) AS YearsSinceLastPromotion#1241, cast(YearsWithCurrManager#44 as int) AS YearsWithCurrManager#1266]\\n:  :  :  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n:  :  +- Project\\n:  :     +- LogicalRDD [BusinessTravel_Non-Travel#610L, BusinessTravel_Travel_Frequently#611L, BusinessTravel_Travel_Rarely#612L, Department_Human Resources#613L, Department_Research & Development#614L, Department_Sales#615L, EducationField_Human Resources#616L, EducationField_Life Sciences#617L, EducationField_Marketing#618L, EducationField_Medical#619L, EducationField_Other#620L, EducationField_Technical Degree#621L, Gender_Female#622L, Gender_Male#623L, JobRole_Healthcare Representative#624L, JobRole_Human Resources#625L, JobRole_Laboratory Technician#626L, JobRole_Manager#627L, JobRole_Manufacturing Director#628L, JobRole_Research Director#629L, JobRole_Research Scientist#630L, JobRole_Sales Executive#631L, JobRole_Sales Representative#632L, MaritalStatus_Divorced#633L, ... 4 more fields], false\\n:  +- Project\\n:     +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n+- Project [cast(BusinessTravel_Non-Travel#1396L as int) AS BusinessTravel_Non-Travel#1452, cast(BusinessTravel_Travel_Frequently#1397L as int) AS BusinessTravel_Travel_Frequently#1481, cast(BusinessTravel_Travel_Rarely#1398L as int) AS BusinessTravel_Travel_Rarely#1510, cast(Department_Human Resources#1399L as int) AS Department_Human Resources#1539, cast(Department_Research & Development#1400L as int) AS Department_Research & Development#1568, cast(Department_Sales#1401L as int) AS Department_Sales#1597, cast(EducationField_Human Resources#1402L as int) AS EducationField_Human Resources#1626, cast(EducationField_Life Sciences#1403L as int) AS EducationField_Life Sciences#1655, cast(EducationField_Marketing#1404L as int) AS EducationField_Marketing#1684, cast(EducationField_Medical#1405L as int) AS EducationField_Medical#1713, cast(EducationField_Other#1406L as int) AS EducationField_Other#1742, cast(EducationField_Technical Degree#1407L as int) AS EducationField_Technical Degree#1771, cast(Gender_Female#1408L as int) AS Gender_Female#1800, cast(Gender_Male#1409L as int) AS Gender_Male#1829, cast(JobRole_Healthcare Representative#1410L as int) AS JobRole_Healthcare Representative#1858, cast(JobRole_Human Resources#1411L as int) AS JobRole_Human Resources#1887, cast(JobRole_Laboratory Technician#1412L as int) AS JobRole_Laboratory Technician#1916, cast(JobRole_Manager#1413L as int) AS JobRole_Manager#1945, cast(JobRole_Manufacturing Director#1414L as int) AS JobRole_Manufacturing Director#1974, cast(JobRole_Research Director#1415L as int) AS JobRole_Research Director#2003, cast(JobRole_Research Scientist#1416L as int) AS JobRole_Research Scientist#2032, cast(JobRole_Sales Executive#1417L as int) AS JobRole_Sales Executive#2061, cast(JobRole_Sales Representative#1418L as int) AS JobRole_Sales Representative#2090, cast(MaritalStatus_Divorced#1419L as int) AS MaritalStatus_Divorced#2119, ... 4 more fields]\\n   +- LogicalRDD [BusinessTravel_Non-Travel#1396L, BusinessTravel_Travel_Frequently#1397L, BusinessTravel_Travel_Rarely#1398L, Department_Human Resources#1399L, Department_Research & Development#1400L, Department_Sales#1401L, EducationField_Human Resources#1402L, EducationField_Life Sciences#1403L, EducationField_Marketing#1404L, EducationField_Medical#1405L, EducationField_Other#1406L, EducationField_Technical Degree#1407L, Gender_Female#1408L, Gender_Male#1409L, JobRole_Healthcare Representative#1410L, JobRole_Human Resources#1411L, JobRole_Laboratory Technician#1412L, JobRole_Manager#1413L, JobRole_Manufacturing Director#1414L, JobRole_Research Director#1415L, JobRole_Research Scientist#1416L, JobRole_Sales Executive#1417L, JobRole_Sales Representative#1418L, MaritalStatus_Divorced#1419L, ... 4 more fields], false\\nand\\nJoin Inner\\n:- Join Inner\\n:  :- Project\\n:  :  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n:  +- Project\\n:     +- LogicalRDD [BusinessTravel_Non-Travel#610L, BusinessTravel_Travel_Frequently#611L, BusinessTravel_Travel_Rarely#612L, Department_Human Resources#613L, Department_Research & Development#614L, Department_Sales#615L, EducationField_Human Resources#616L, EducationField_Life Sciences#617L, EducationField_Marketing#618L, EducationField_Medical#619L, EducationField_Other#620L, EducationField_Technical Degree#621L, Gender_Female#622L, Gender_Male#623L, JobRole_Healthcare Representative#624L, JobRole_Human Resources#625L, JobRole_Laboratory Technician#626L, JobRole_Manager#627L, JobRole_Manufacturing Director#628L, JobRole_Research Director#629L, JobRole_Research Scientist#630L, JobRole_Sales Executive#631L, JobRole_Sales Representative#632L, MaritalStatus_Divorced#633L, ... 4 more fields], false\\n+- Project [bool_to_int(Attrition#11) AS Attrition_numerical#222]\\n   +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o489.count.\n: org.apache.spark.sql.AnalysisException: Detected implicit cartesian product for INNER join between logical plans\nJoin Inner\n:- Join Inner\n:  :- Join Inner\n:  :  :- Project [cast(Age#10 as int) AS Age#691, cast(DailyRate#13 as int) AS DailyRate#716, cast(DistanceFromHome#15 as int) AS DistanceFromHome#741, cast(Education#16 as int) AS Education#766, cast(EmployeeNumber#19 as int) AS EmployeeNumber#791, cast(EnvironmentSatisfaction#20 as int) AS EnvironmentSatisfaction#816, cast(HourlyRate#22 as int) AS HourlyRate#841, cast(JobInvolvement#23 as int) AS JobInvolvement#866, cast(JobLevel#24 as int) AS JobLevel#891, cast(JobSatisfaction#26 as int) AS JobSatisfaction#916, cast(MonthlyIncome#28 as int) AS MonthlyIncome#941, cast(MonthlyRate#29 as int) AS MonthlyRate#966, cast(NumCompaniesWorked#30 as int) AS NumCompaniesWorked#991, cast(PercentSalaryHike#33 as int) AS PercentSalaryHike#1016, cast(PerformanceRating#34 as int) AS PerformanceRating#1041, cast(RelationshipSatisfaction#35 as int) AS RelationshipSatisfaction#1066, cast(StockOptionLevel#37 as int) AS StockOptionLevel#1091, cast(TotalWorkingYears#38 as int) AS TotalWorkingYears#1116, cast(TrainingTimesLastYear#39 as int) AS TrainingTimesLastYear#1141, cast(WorkLifeBalance#40 as int) AS WorkLifeBalance#1166, cast(YearsAtCompany#41 as int) AS YearsAtCompany#1191, cast(YearsInCurrentRole#42 as int) AS YearsInCurrentRole#1216, cast(YearsSinceLastPromotion#43 as int) AS YearsSinceLastPromotion#1241, cast(YearsWithCurrManager#44 as int) AS YearsWithCurrManager#1266]\n:  :  :  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\n:  :  +- Project\n:  :     +- LogicalRDD [BusinessTravel_Non-Travel#610L, BusinessTravel_Travel_Frequently#611L, BusinessTravel_Travel_Rarely#612L, Department_Human Resources#613L, Department_Research & Development#614L, Department_Sales#615L, EducationField_Human Resources#616L, EducationField_Life Sciences#617L, EducationField_Marketing#618L, EducationField_Medical#619L, EducationField_Other#620L, EducationField_Technical Degree#621L, Gender_Female#622L, Gender_Male#623L, JobRole_Healthcare Representative#624L, JobRole_Human Resources#625L, JobRole_Laboratory Technician#626L, JobRole_Manager#627L, JobRole_Manufacturing Director#628L, JobRole_Research Director#629L, JobRole_Research Scientist#630L, JobRole_Sales Executive#631L, JobRole_Sales Representative#632L, MaritalStatus_Divorced#633L, ... 4 more fields], false\n:  +- Project\n:     +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\n+- Project [cast(BusinessTravel_Non-Travel#1396L as int) AS BusinessTravel_Non-Travel#1452, cast(BusinessTravel_Travel_Frequently#1397L as int) AS BusinessTravel_Travel_Frequently#1481, cast(BusinessTravel_Travel_Rarely#1398L as int) AS BusinessTravel_Travel_Rarely#1510, cast(Department_Human Resources#1399L as int) AS Department_Human Resources#1539, cast(Department_Research & Development#1400L as int) AS Department_Research & Development#1568, cast(Department_Sales#1401L as int) AS Department_Sales#1597, cast(EducationField_Human Resources#1402L as int) AS EducationField_Human Resources#1626, cast(EducationField_Life Sciences#1403L as int) AS EducationField_Life Sciences#1655, cast(EducationField_Marketing#1404L as int) AS EducationField_Marketing#1684, cast(EducationField_Medical#1405L as int) AS EducationField_Medical#1713, cast(EducationField_Other#1406L as int) AS EducationField_Other#1742, cast(EducationField_Technical Degree#1407L as int) AS EducationField_Technical Degree#1771, cast(Gender_Female#1408L as int) AS Gender_Female#1800, cast(Gender_Male#1409L as int) AS Gender_Male#1829, cast(JobRole_Healthcare Representative#1410L as int) AS JobRole_Healthcare Representative#1858, cast(JobRole_Human Resources#1411L as int) AS JobRole_Human Resources#1887, cast(JobRole_Laboratory Technician#1412L as int) AS JobRole_Laboratory Technician#1916, cast(JobRole_Manager#1413L as int) AS JobRole_Manager#1945, cast(JobRole_Manufacturing Director#1414L as int) AS JobRole_Manufacturing Director#1974, cast(JobRole_Research Director#1415L as int) AS JobRole_Research Director#2003, cast(JobRole_Research Scientist#1416L as int) AS JobRole_Research Scientist#2032, cast(JobRole_Sales Executive#1417L as int) AS JobRole_Sales Executive#2061, cast(JobRole_Sales Representative#1418L as int) AS JobRole_Sales Representative#2090, cast(MaritalStatus_Divorced#1419L as int) AS MaritalStatus_Divorced#2119, ... 4 more fields]\n   +- LogicalRDD [BusinessTravel_Non-Travel#1396L, BusinessTravel_Travel_Frequently#1397L, BusinessTravel_Travel_Rarely#1398L, Department_Human Resources#1399L, Department_Research & Development#1400L, Department_Sales#1401L, EducationField_Human Resources#1402L, EducationField_Life Sciences#1403L, EducationField_Marketing#1404L, EducationField_Medical#1405L, EducationField_Other#1406L, EducationField_Technical Degree#1407L, Gender_Female#1408L, Gender_Male#1409L, JobRole_Healthcare Representative#1410L, JobRole_Human Resources#1411L, JobRole_Laboratory Technician#1412L, JobRole_Manager#1413L, JobRole_Manufacturing Director#1414L, JobRole_Research Director#1415L, JobRole_Research Scientist#1416L, JobRole_Sales Executive#1417L, JobRole_Sales Representative#1418L, MaritalStatus_Divorced#1419L, ... 4 more fields], false\nand\nJoin Inner\n:- Join Inner\n:  :- Project\n:  :  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\n:  +- Project\n:     +- LogicalRDD [BusinessTravel_Non-Travel#610L, BusinessTravel_Travel_Frequently#611L, BusinessTravel_Travel_Rarely#612L, Department_Human Resources#613L, Department_Research & Development#614L, Department_Sales#615L, EducationField_Human Resources#616L, EducationField_Life Sciences#617L, EducationField_Marketing#618L, EducationField_Medical#619L, EducationField_Other#620L, EducationField_Technical Degree#621L, Gender_Female#622L, Gender_Male#623L, JobRole_Healthcare Representative#624L, JobRole_Human Resources#625L, JobRole_Laboratory Technician#626L, JobRole_Manager#627L, JobRole_Manufacturing Director#628L, JobRole_Research Director#629L, JobRole_Research Scientist#630L, JobRole_Sales Executive#631L, JobRole_Sales Representative#632L, MaritalStatus_Divorced#633L, ... 4 more fields], false\n+- Project [bool_to_int(Attrition#11) AS Attrition_numerical#222]\n   +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\nJoin condition is missing or trivial.\nEither: use the CROSS JOIN syntax to allow cartesian products between these\nrelations, or: enable implicit cartesian products by setting the configuration\nvariable spark.sql.crossJoin.enabled=true;\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1124)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$$anonfun$apply$21.applyOrElse(Optimizer.scala:1121)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:267)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:266)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:272)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:256)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1121)\r\n\tat org.apache.spark.sql.catalyst.optimizer.CheckCartesianProducts$.apply(Optimizer.scala:1103)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57)\r\n\tat scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66)\r\n\tat scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)\r\n\tat scala.collection.immutable.List.foreach(List.scala:381)\r\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:66)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:72)\r\n\tat org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:68)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:77)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3249)\r\n\tat org.apache.spark.sql.Dataset.count(Dataset.scala:2769)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-b6b4a281087c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0massembled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massembler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mibm_hr_target\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0massembled\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandomSplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m13234\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mtraining_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    453\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m         \"\"\"\n\u001b[1;32m--> 455\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mignore_unicode_prefix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\spark-2.3.1-bin-hadoop2.7\\python\\lib\\py4j-0.10.7-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:/spark-2.3.1-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: 'Detected implicit cartesian product for INNER join between logical plans\\nJoin Inner\\n:- Join Inner\\n:  :- Join Inner\\n:  :  :- Project [cast(Age#10 as int) AS Age#691, cast(DailyRate#13 as int) AS DailyRate#716, cast(DistanceFromHome#15 as int) AS DistanceFromHome#741, cast(Education#16 as int) AS Education#766, cast(EmployeeNumber#19 as int) AS EmployeeNumber#791, cast(EnvironmentSatisfaction#20 as int) AS EnvironmentSatisfaction#816, cast(HourlyRate#22 as int) AS HourlyRate#841, cast(JobInvolvement#23 as int) AS JobInvolvement#866, cast(JobLevel#24 as int) AS JobLevel#891, cast(JobSatisfaction#26 as int) AS JobSatisfaction#916, cast(MonthlyIncome#28 as int) AS MonthlyIncome#941, cast(MonthlyRate#29 as int) AS MonthlyRate#966, cast(NumCompaniesWorked#30 as int) AS NumCompaniesWorked#991, cast(PercentSalaryHike#33 as int) AS PercentSalaryHike#1016, cast(PerformanceRating#34 as int) AS PerformanceRating#1041, cast(RelationshipSatisfaction#35 as int) AS RelationshipSatisfaction#1066, cast(StockOptionLevel#37 as int) AS StockOptionLevel#1091, cast(TotalWorkingYears#38 as int) AS TotalWorkingYears#1116, cast(TrainingTimesLastYear#39 as int) AS TrainingTimesLastYear#1141, cast(WorkLifeBalance#40 as int) AS WorkLifeBalance#1166, cast(YearsAtCompany#41 as int) AS YearsAtCompany#1191, cast(YearsInCurrentRole#42 as int) AS YearsInCurrentRole#1216, cast(YearsSinceLastPromotion#43 as int) AS YearsSinceLastPromotion#1241, cast(YearsWithCurrManager#44 as int) AS YearsWithCurrManager#1266]\\n:  :  :  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n:  :  +- Project\\n:  :     +- LogicalRDD [BusinessTravel_Non-Travel#610L, BusinessTravel_Travel_Frequently#611L, BusinessTravel_Travel_Rarely#612L, Department_Human Resources#613L, Department_Research & Development#614L, Department_Sales#615L, EducationField_Human Resources#616L, EducationField_Life Sciences#617L, EducationField_Marketing#618L, EducationField_Medical#619L, EducationField_Other#620L, EducationField_Technical Degree#621L, Gender_Female#622L, Gender_Male#623L, JobRole_Healthcare Representative#624L, JobRole_Human Resources#625L, JobRole_Laboratory Technician#626L, JobRole_Manager#627L, JobRole_Manufacturing Director#628L, JobRole_Research Director#629L, JobRole_Research Scientist#630L, JobRole_Sales Executive#631L, JobRole_Sales Representative#632L, MaritalStatus_Divorced#633L, ... 4 more fields], false\\n:  +- Project\\n:     +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n+- Project [cast(BusinessTravel_Non-Travel#1396L as int) AS BusinessTravel_Non-Travel#1452, cast(BusinessTravel_Travel_Frequently#1397L as int) AS BusinessTravel_Travel_Frequently#1481, cast(BusinessTravel_Travel_Rarely#1398L as int) AS BusinessTravel_Travel_Rarely#1510, cast(Department_Human Resources#1399L as int) AS Department_Human Resources#1539, cast(Department_Research & Development#1400L as int) AS Department_Research & Development#1568, cast(Department_Sales#1401L as int) AS Department_Sales#1597, cast(EducationField_Human Resources#1402L as int) AS EducationField_Human Resources#1626, cast(EducationField_Life Sciences#1403L as int) AS EducationField_Life Sciences#1655, cast(EducationField_Marketing#1404L as int) AS EducationField_Marketing#1684, cast(EducationField_Medical#1405L as int) AS EducationField_Medical#1713, cast(EducationField_Other#1406L as int) AS EducationField_Other#1742, cast(EducationField_Technical Degree#1407L as int) AS EducationField_Technical Degree#1771, cast(Gender_Female#1408L as int) AS Gender_Female#1800, cast(Gender_Male#1409L as int) AS Gender_Male#1829, cast(JobRole_Healthcare Representative#1410L as int) AS JobRole_Healthcare Representative#1858, cast(JobRole_Human Resources#1411L as int) AS JobRole_Human Resources#1887, cast(JobRole_Laboratory Technician#1412L as int) AS JobRole_Laboratory Technician#1916, cast(JobRole_Manager#1413L as int) AS JobRole_Manager#1945, cast(JobRole_Manufacturing Director#1414L as int) AS JobRole_Manufacturing Director#1974, cast(JobRole_Research Director#1415L as int) AS JobRole_Research Director#2003, cast(JobRole_Research Scientist#1416L as int) AS JobRole_Research Scientist#2032, cast(JobRole_Sales Executive#1417L as int) AS JobRole_Sales Executive#2061, cast(JobRole_Sales Representative#1418L as int) AS JobRole_Sales Representative#2090, cast(MaritalStatus_Divorced#1419L as int) AS MaritalStatus_Divorced#2119, ... 4 more fields]\\n   +- LogicalRDD [BusinessTravel_Non-Travel#1396L, BusinessTravel_Travel_Frequently#1397L, BusinessTravel_Travel_Rarely#1398L, Department_Human Resources#1399L, Department_Research & Development#1400L, Department_Sales#1401L, EducationField_Human Resources#1402L, EducationField_Life Sciences#1403L, EducationField_Marketing#1404L, EducationField_Medical#1405L, EducationField_Other#1406L, EducationField_Technical Degree#1407L, Gender_Female#1408L, Gender_Male#1409L, JobRole_Healthcare Representative#1410L, JobRole_Human Resources#1411L, JobRole_Laboratory Technician#1412L, JobRole_Manager#1413L, JobRole_Manufacturing Director#1414L, JobRole_Research Director#1415L, JobRole_Research Scientist#1416L, JobRole_Sales Executive#1417L, JobRole_Sales Representative#1418L, MaritalStatus_Divorced#1419L, ... 4 more fields], false\\nand\\nJoin Inner\\n:- Join Inner\\n:  :- Project\\n:  :  +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\n:  +- Project\\n:     +- LogicalRDD [BusinessTravel_Non-Travel#610L, BusinessTravel_Travel_Frequently#611L, BusinessTravel_Travel_Rarely#612L, Department_Human Resources#613L, Department_Research & Development#614L, Department_Sales#615L, EducationField_Human Resources#616L, EducationField_Life Sciences#617L, EducationField_Marketing#618L, EducationField_Medical#619L, EducationField_Other#620L, EducationField_Technical Degree#621L, Gender_Female#622L, Gender_Male#623L, JobRole_Healthcare Representative#624L, JobRole_Human Resources#625L, JobRole_Laboratory Technician#626L, JobRole_Manager#627L, JobRole_Manufacturing Director#628L, JobRole_Research Director#629L, JobRole_Research Scientist#630L, JobRole_Sales Executive#631L, JobRole_Sales Representative#632L, MaritalStatus_Divorced#633L, ... 4 more fields], false\\n+- Project [bool_to_int(Attrition#11) AS Attrition_numerical#222]\\n   +- Relation[Age#10,Attrition#11,BusinessTravel#12,DailyRate#13,Department#14,DistanceFromHome#15,Education#16,EducationField#17,EmployeeCount#18,EmployeeNumber#19,EnvironmentSatisfaction#20,Gender#21,HourlyRate#22,JobInvolvement#23,JobLevel#24,JobRole#25,JobSatisfaction#26,MaritalStatus#27,MonthlyIncome#28,MonthlyRate#29,NumCompaniesWorked#30,Over18#31,OverTime#32,PercentSalaryHike#33,... 11 more fields] csv\\nJoin condition is missing or trivial.\\nEither: use the CROSS JOIN syntax to allow cartesian products between these\\nrelations, or: enable implicit cartesian products by setting the configuration\\nvariable spark.sql.crossJoin.enabled=true;'"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer\n",
    "assembler = VectorAssembler(inputCols = feature_col, outputCol = \"features\")\n",
    "assembled = assembler.transform(ibm_hr_target)\n",
    "(training_data, testing_data) = assembled.randomSplit([0.8, 0.2], seed = 13234)\n",
    "training_data.count(), test_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
